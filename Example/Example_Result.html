<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 600px;
                 background-color: #222222;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 600px;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#97c2fc", "font": {"color": "white"}, "id": "LoRA: Low-Rank Adaptation of Large Language Models", "label": "LoRA: Low-Rank Adaptation of Large Language Models", "shape": "dot", "size": 10, "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "label": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "shape": "dot", "size": 5, "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "The Power of Scale for Parameter-Efficient Prompt Tuning", "label": "The Power of Scale for Parameter-Efficient Prompt Tuning", "shape": "dot", "size": 10, "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "label": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "shape": "dot", "size": 10, "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "label": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "shape": "dot", "size": 155, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning multiple visual domains with residual adapters", "label": "Learning multiple visual domains with residual adapters", "shape": "dot", "size": 15, "title": "Learning multiple visual domains with residual adapters"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "DART: Open-Domain Structured Data Record to Text Generation", "label": "DART: Open-Domain Structured Data Record to Text Generation", "shape": "dot", "size": 10, "title": "DART: Open-Domain Structured Data Record to Text Generation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Parameter-Efficient Transfer Learning for NLP", "label": "Parameter-Efficient Transfer Learning for NLP", "shape": "dot", "size": 20, "title": "Parameter-Efficient Transfer Learning for NLP"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "label": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "shape": "dot", "size": 15, "title": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Neural Network Acceptability Judgments", "label": "Neural Network Acceptability Judgments", "shape": "dot", "size": 45, "title": "Neural Network Acceptability Judgments"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "The E2E Dataset: New Challenges For End-to-End Generation", "label": "The E2E Dataset: New Challenges For End-to-End Generation", "shape": "dot", "size": 15, "title": "The E2E Dataset: New Challenges For End-to-End Generation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "When Do Neural Networks Outperform Kernel Methods?", "label": "When Do Neural Networks Outperform Kernel Methods?", "shape": "dot", "size": 10, "title": "When Do Neural Networks Outperform Kernel Methods?"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Decoupled Weight Decay Regularization", "label": "Decoupled Weight Decay Regularization", "shape": "dot", "size": 35, "title": "Decoupled Weight Decay Regularization"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "WARP: Word-level Adversarial ReProgramming", "label": "WARP: Word-level Adversarial ReProgramming", "shape": "dot", "size": 10, "title": "WARP: Word-level Adversarial ReProgramming"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "label": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "shape": "dot", "size": 25, "title": "Feature Purification: How Adversarial Training Performs Robust Deep Learning"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Feature Learning in Infinite-Width Neural Networks", "label": "Feature Learning in Infinite-Width Neural Networks", "shape": "dot", "size": 15, "title": "Feature Learning in Infinite-Width Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "label": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "shape": "dot", "size": 20, "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "GPT Understands, Too", "label": "GPT Understands, Too", "shape": "dot", "size": 10, "title": "GPT Understands, Too"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Measuring the Intrinsic Dimension of Objective Landscapes", "label": "Measuring the Intrinsic Dimension of Objective Landscapes", "shape": "dot", "size": 15, "title": "Measuring the Intrinsic Dimension of Objective Landscapes"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "CNN Features off-the-shelf: an Astounding Baseline for Recognition", "label": "CNN Features off-the-shelf: an Astounding Baseline for Recognition", "shape": "dot", "size": 5, "title": "CNN Features off-the-shelf: an Astounding Baseline for Recognition"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "label": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "shape": "dot", "size": 40, "title": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Maxout Networks", "label": "Maxout Networks", "shape": "dot", "size": 10, "title": "Maxout Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Language Models are Few-Shot Learners", "label": "Language Models are Few-Shot Learners", "shape": "dot", "size": 55, "title": "Language Models are Few-Shot Learners"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Improving neural networks by preventing co-adaptation of feature detectors", "label": "Improving neural networks by preventing co-adaptation of feature detectors", "shape": "dot", "size": 35, "title": "Improving neural networks by preventing co-adaptation of feature detectors"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "DeepPose: Human Pose Estimation via Deep Neural Networks", "label": "DeepPose: Human Pose Estimation via Deep Neural Networks", "shape": "dot", "size": 5, "title": "DeepPose: Human Pose Estimation via Deep Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "label": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "shape": "dot", "size": 5, "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers", "label": "Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers", "shape": "dot", "size": 5, "title": "Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Better Fine-Tuning by Reducing Representational Collapse", "label": "Better Fine-Tuning by Reducing Representational Collapse", "shape": "dot", "size": 10, "title": "Better Fine-Tuning by Reducing Representational Collapse"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "label": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "shape": "dot", "size": 10, "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Character-Level Language Modeling with Deeper Self-Attention", "label": "Character-Level Language Modeling with Deeper Self-Attention", "shape": "dot", "size": 25, "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "label": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "shape": "dot", "size": 20, "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "MaskGAN: Better Text Generation via Filling in the______", "label": "MaskGAN: Better Text Generation via Filling in the______", "shape": "dot", "size": 15, "title": "MaskGAN: Better Text Generation via Filling in the______"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Unsupervised Cross-lingual Representation Learning at Scale", "label": "Unsupervised Cross-lingual Representation Learning at Scale", "shape": "dot", "size": 20, "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Universal Sentence Encoder", "label": "Universal Sentence Encoder", "shape": "dot", "size": 15, "title": "Universal Sentence Encoder"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Incremental Learning Through Deep Adaptation", "label": "Incremental Learning Through Deep Adaptation", "shape": "dot", "size": 10, "title": "Incremental Learning Through Deep Adaptation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Stronger generalization bounds for deep nets via a compression approach", "label": "Stronger generalization bounds for deep nets via a compression approach", "shape": "dot", "size": 40, "title": "Stronger generalization bounds for deep nets via a compression approach"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Transforming Question Answering Datasets Into Natural Language Inference Datasets", "label": "Transforming Question Answering Datasets Into Natural Language Inference Datasets", "shape": "dot", "size": 15, "title": "Transforming Question Answering Datasets Into Natural Language Inference Datasets"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "What Does BERT Look At? An Analysis of BERT\u0027s Attention", "label": "What Does BERT Look At? An Analysis of BERT\u0027s Attention", "shape": "dot", "size": 25, "title": "What Does BERT Look At? An Analysis of BERT\u0027s Attention"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "label": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "shape": "dot", "size": 45, "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings", "label": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings", "shape": "dot", "size": 15, "title": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "label": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "shape": "dot", "size": 20, "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "label": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "shape": "dot", "size": 165, "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Universal representations:The missing link between faces, text, planktons, and cat breeds", "label": "Universal representations:The missing link between faces, text, planktons, and cat breeds", "shape": "dot", "size": 10, "title": "Universal representations:The missing link between faces, text, planktons, and cat breeds"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Reconciling modern machine learning practice and the bias-variance trade-off", "label": "Reconciling modern machine learning practice and the bias-variance trade-off", "shape": "dot", "size": 35, "title": "Reconciling modern machine learning practice and the bias-variance trade-off"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Evaluating Lottery Tickets Under Distributional Shifts", "label": "Evaluating Lottery Tickets Under Distributional Shifts", "shape": "dot", "size": 10, "title": "Evaluating Lottery Tickets Under Distributional Shifts"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "label": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "shape": "dot", "size": 60, "title": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Text-to-Text Pre-Training for Data-to-Text Tasks", "label": "Text-to-Text Pre-Training for Data-to-Text Tasks", "shape": "dot", "size": 10, "title": "Text-to-Text Pre-Training for Data-to-Text Tasks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "AutoAugment: Learning Augmentation Policies from Data", "label": "AutoAugment: Learning Augmentation Policies from Data", "shape": "dot", "size": 20, "title": "AutoAugment: Learning Augmentation Policies from Data"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "label": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "shape": "dot", "size": 65, "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "On Lazy Training in Differentiable Programming", "label": "On Lazy Training in Differentiable Programming", "shape": "dot", "size": 90, "title": "On Lazy Training in Differentiable Programming"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity", "label": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity", "shape": "dot", "size": 10, "title": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "DenseNet: Implementing Efficient ConvNet Descriptor Pyramids", "label": "DenseNet: Implementing Efficient ConvNet Descriptor Pyramids", "shape": "dot", "size": 10, "title": "DenseNet: Implementing Efficient ConvNet Descriptor Pyramids"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Disentangling feature and lazy training in deep neural networks", "label": "Disentangling feature and lazy training in deep neural networks", "shape": "dot", "size": 20, "title": "Disentangling feature and lazy training in deep neural networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits", "label": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits", "shape": "dot", "size": 20, "title": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Convergence Theory for Deep Learning via Over-Parameterization", "label": "A Convergence Theory for Deep Learning via Over-Parameterization", "shape": "dot", "size": 135, "title": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport", "label": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport", "shape": "dot", "size": 55, "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "label": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "shape": "dot", "size": 105, "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "label": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "shape": "dot", "size": 45, "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Theoretical properties of the global optimizer of two layer neural network", "label": "Theoretical properties of the global optimizer of two layer neural network", "shape": "dot", "size": 30, "title": "Theoretical properties of the global optimizer of two layer neural network"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Progressive Neural Networks", "label": "Progressive Neural Networks", "shape": "dot", "size": 20, "title": "Progressive Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "label": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "shape": "dot", "size": 10, "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency", "label": "RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency", "shape": "dot", "size": 10, "title": "RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "In Defense of the Triplet Loss for Person Re-Identification", "label": "In Defense of the Triplet Loss for Person Re-Identification", "shape": "dot", "size": 10, "title": "In Defense of the Triplet Loss for Person Re-Identification"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "GeDi: Generative Discriminator Guided Sequence Generation", "label": "GeDi: Generative Discriminator Guided Sequence Generation", "shape": "dot", "size": 10, "title": "GeDi: Generative Discriminator Guided Sequence Generation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning", "label": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning", "shape": "dot", "size": 10, "title": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Explaining and Harnessing Adversarial Examples", "label": "Explaining and Harnessing Adversarial Examples", "shape": "dot", "size": 55, "title": "Explaining and Harnessing Adversarial Examples"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Annotation Artifacts in Natural Language Inference Data", "label": "Annotation Artifacts in Natural Language Inference Data", "shape": "dot", "size": 15, "title": "Annotation Artifacts in Natural Language Inference Data"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "What Makes Good In-Context Examples for GPT-$3$?", "label": "What Makes Good In-Context Examples for GPT-$3$?", "shape": "dot", "size": 10, "title": "What Makes Good In-Context Examples for GPT-$3$?"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "label": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "shape": "dot", "size": 65, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks", "label": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks", "shape": "dot", "size": 40, "title": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u003c0.5MB model size", "label": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u003c0.5MB model size", "shape": "dot", "size": 15, "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u003c0.5MB model size"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Risk and parameter convergence of logistic regression", "label": "Risk and parameter convergence of logistic regression", "shape": "dot", "size": 20, "title": "Risk and parameter convergence of logistic regression"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Generalization bounds for deep convolutional neural networks", "label": "Generalization bounds for deep convolutional neural networks", "shape": "dot", "size": 10, "title": "Generalization bounds for deep convolutional neural networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "label": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "shape": "dot", "size": 115, "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Making Pre-trained Language Models Better Few-shot Learners", "label": "Making Pre-trained Language Models Better Few-shot Learners", "shape": "dot", "size": 15, "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Linearized two-layers neural networks in high dimension", "label": "Linearized two-layers neural networks in high dimension", "shape": "dot", "size": 55, "title": "Linearized two-layers neural networks in high dimension"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild", "label": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild", "shape": "dot", "size": 5, "title": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "label": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "shape": "dot", "size": 10, "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Do Better ImageNet Models Transfer Better?", "label": "Do Better ImageNet Models Transfer Better?", "shape": "dot", "size": 20, "title": "Do Better ImageNet Models Transfer Better?"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "The Marginal Value of Adaptive Gradient Methods in Machine Learning", "label": "The Marginal Value of Adaptive Gradient Methods in Machine Learning", "shape": "dot", "size": 10, "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "label": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "shape": "dot", "size": 10, "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Normalized Direction-preserving Adam", "label": "Normalized Direction-preserving Adam", "shape": "dot", "size": 10, "title": "Normalized Direction-preserving Adam"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Adversarial examples from computational constraints", "label": "Adversarial examples from computational constraints", "shape": "dot", "size": 25, "title": "Adversarial examples from computational constraints"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Countering Adversarial Images using Input Transformations", "label": "Countering Adversarial Images using Input Transformations", "shape": "dot", "size": 25, "title": "Countering Adversarial Images using Input Transformations"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "label": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "shape": "dot", "size": 45, "title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks", "label": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks", "shape": "dot", "size": 35, "title": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "What makes ImageNet good for transfer learning?", "label": "What makes ImageNet good for transfer learning?", "shape": "dot", "size": 25, "title": "What makes ImageNet good for transfer learning?"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "SGDR: Stochastic Gradient Descent with Warm Restarts", "label": "SGDR: Stochastic Gradient Descent with Warm Restarts", "shape": "dot", "size": 40, "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "label": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "shape": "dot", "size": 65, "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "label": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "shape": "dot", "size": 25, "title": "Finite Depth and Width Corrections to the Neural Tangent Kernel"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Gradient Descent Finds Global Minima of Deep Neural Networks", "label": "Gradient Descent Finds Global Minima of Deep Neural Networks", "shape": "dot", "size": 125, "title": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Snapshot Ensembles: Train 1, get M for free", "label": "Snapshot Ensembles: Train 1, get M for free", "shape": "dot", "size": 10, "title": "Snapshot Ensembles: Train 1, get M for free"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs", "label": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs", "shape": "dot", "size": 95, "title": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Two models of double descent for weak features", "label": "Two models of double descent for weak features", "shape": "dot", "size": 25, "title": "Two models of double descent for weak features"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Size-Independent Sample Complexity of Neural Networks", "label": "Size-Independent Sample Complexity of Neural Networks", "shape": "dot", "size": 35, "title": "Size-Independent Sample Complexity of Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Character-level Convolutional Networks for Text Classification", "label": "Character-level Convolutional Networks for Text Classification", "shape": "dot", "size": 10, "title": "Character-level Convolutional Networks for Text Classification"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Three Mechanisms of Weight Decay Regularization", "label": "Three Mechanisms of Weight Decay Regularization", "shape": "dot", "size": 10, "title": "Three Mechanisms of Weight Decay Regularization"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Sharp Minima Can Generalize For Deep Nets", "label": "Sharp Minima Can Generalize For Deep Nets", "shape": "dot", "size": 20, "title": "Sharp Minima Can Generalize For Deep Nets"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "label": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "shape": "dot", "size": 10, "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "UNITER: UNiversal Image-TExt Representation Learning", "label": "UNITER: UNiversal Image-TExt Representation Learning", "shape": "dot", "size": 10, "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality", "label": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality", "shape": "dot", "size": 15, "title": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "label": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "shape": "dot", "size": 20, "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "GLTR: Statistical Detection and Visualization of Generated Text", "label": "GLTR: Statistical Detection and Visualization of Generated Text", "shape": "dot", "size": 10, "title": "GLTR: Statistical Detection and Visualization of Generated Text"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "label": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "shape": "dot", "size": 55, "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "On Exact Computation with an Infinitely Wide Neural Net", "label": "On Exact Computation with an Infinitely Wide Neural Net", "shape": "dot", "size": 65, "title": "On Exact Computation with an Infinitely Wide Neural Net"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Making Method of Moments Great Again? -- How can GANs learn distributions", "label": "Making Method of Moments Great Again? -- How can GANs learn distributions", "shape": "dot", "size": 20, "title": "Making Method of Moments Great Again? -- How can GANs learn distributions"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data", "label": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data", "shape": "dot", "size": 30, "title": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "SFace: An Efficient Network for Face Detection in Large Scale Variations", "label": "SFace: An Efficient Network for Face Detection in Large Scale Variations", "shape": "dot", "size": 10, "title": "SFace: An Efficient Network for Face Detection in Large Scale Variations"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "label": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "shape": "dot", "size": 20, "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "label": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "shape": "dot", "size": 10, "title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "label": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "shape": "dot", "size": 10, "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Why bigger is not always better: on finite and infinite neural networks", "label": "Why bigger is not always better: on finite and infinite neural networks", "shape": "dot", "size": 15, "title": "Why bigger is not always better: on finite and infinite neural networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "label": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "shape": "dot", "size": 20, "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Experience Grounds Language", "label": "Experience Grounds Language", "shape": "dot", "size": 10, "title": "Experience Grounds Language"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Enhanced Convolutional Neural Tangent Kernels", "label": "Enhanced Convolutional Neural Tangent Kernels", "shape": "dot", "size": 25, "title": "Enhanced Convolutional Neural Tangent Kernels"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Multi-domain Neural Network Language Generation for Spoken Dialogue Systems", "label": "Multi-domain Neural Network Language Generation for Spoken Dialogue Systems", "shape": "dot", "size": 10, "title": "Multi-domain Neural Network Language Generation for Spoken Dialogue Systems"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Cyclical Learning Rates for Training Neural Networks", "label": "Cyclical Learning Rates for Training Neural Networks", "shape": "dot", "size": 20, "title": "Cyclical Learning Rates for Training Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", "label": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", "shape": "dot", "size": 50, "title": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Adam: A Method for Stochastic Optimization", "label": "Adam: A Method for Stochastic Optimization", "shape": "dot", "size": 80, "title": "Adam: A Method for Stochastic Optimization"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Multiscale Visualization of Attention in the Transformer Model", "label": "A Multiscale Visualization of Attention in the Transformer Model", "shape": "dot", "size": 10, "title": "A Multiscale Visualization of Attention in the Transformer Model"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Deep kernel processes", "label": "Deep kernel processes", "shape": "dot", "size": 10, "title": "Deep kernel processes"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach", "label": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach", "shape": "dot", "size": 95, "title": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning One-hidden-layer Neural Networks with Landscape Design", "label": "Learning One-hidden-layer Neural Networks with Landscape Design", "shape": "dot", "size": 65, "title": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "label": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "shape": "dot", "size": 65, "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "REALM: Retrieval-Augmented Language Model Pre-Training", "label": "REALM: Retrieval-Augmented Language Model Pre-Training", "shape": "dot", "size": 30, "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Are adversarial examples inevitable?", "label": "Are adversarial examples inevitable?", "shape": "dot", "size": 10, "title": "Are adversarial examples inevitable?"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Adversarial Training Can Hurt Generalization", "label": "Adversarial Training Can Hurt Generalization", "shape": "dot", "size": 20, "title": "Adversarial Training Can Hurt Generalization"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Precise Tradeoffs in Adversarial Training for Linear Regression", "label": "Precise Tradeoffs in Adversarial Training for Linear Regression", "shape": "dot", "size": 10, "title": "Precise Tradeoffs in Adversarial Training for Linear Regression"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "No bad local minima: Data independent training error guarantees for multilayer neural networks", "label": "No bad local minima: Data independent training error guarantees for multilayer neural networks", "shape": "dot", "size": 120, "title": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples", "label": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples", "shape": "dot", "size": 20, "title": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Language Models are Open Knowledge Graphs", "label": "Language Models are Open Knowledge Graphs", "shape": "dot", "size": 10, "title": "Language Models are Open Knowledge Graphs"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Meta-Learning for Low-Resource Neural Machine Translation", "label": "Meta-Learning for Low-Resource Neural Machine Translation", "shape": "dot", "size": 10, "title": "Meta-Learning for Low-Resource Neural Machine Translation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Train faster, generalize better: Stability of stochastic gradient descent", "label": "Train faster, generalize better: Stability of stochastic gradient descent", "shape": "dot", "size": 35, "title": "Train faster, generalize better: Stability of stochastic gradient descent"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Gradient descent aligns the layers of deep linear networks", "label": "Gradient descent aligns the layers of deep linear networks", "shape": "dot", "size": 25, "title": "Gradient descent aligns the layers of deep linear networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment", "label": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment", "shape": "dot", "size": 20, "title": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "U-Net: Machine Reading Comprehension with Unanswerable Questions", "label": "U-Net: Machine Reading Comprehension with Unanswerable Questions", "shape": "dot", "size": 10, "title": "U-Net: Machine Reading Comprehension with Unanswerable Questions"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "label": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "shape": "dot", "size": 10, "title": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples", "label": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples", "shape": "dot", "size": 30, "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "It\u0027s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners", "label": "It\u0027s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners", "shape": "dot", "size": 15, "title": "It\u0027s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "PIQA: Reasoning about Physical Commonsense in Natural Language", "label": "PIQA: Reasoning about Physical Commonsense in Natural Language", "shape": "dot", "size": 10, "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Wide Residual Networks", "label": "Wide Residual Networks", "shape": "dot", "size": 70, "title": "Wide Residual Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them", "label": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them", "shape": "dot", "size": 15, "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Revisiting Adversarial Risk", "label": "Revisiting Adversarial Risk", "shape": "dot", "size": 15, "title": "Revisiting Adversarial Risk"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Pretrained Transformers Improve Out-of-Distribution Robustness", "label": "Pretrained Transformers Improve Out-of-Distribution Robustness", "shape": "dot", "size": 10, "title": "Pretrained Transformers Improve Out-of-Distribution Robustness"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Crowd-sourcing NLG Data: Pictures Elicit Better Data", "label": "Crowd-sourcing NLG Data: Pictures Elicit Better Data", "shape": "dot", "size": 10, "title": "Crowd-sourcing NLG Data: Pictures Elicit Better Data"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Context-aware Natural Language Generator for Dialogue Systems", "label": "A Context-aware Natural Language Generator for Dialogue Systems", "shape": "dot", "size": 10, "title": "A Context-aware Natural Language Generator for Dialogue Systems"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Shake-Shake regularization", "label": "Shake-Shake regularization", "shape": "dot", "size": 15, "title": "Shake-Shake regularization"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time", "label": "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time", "shape": "dot", "size": 30, "title": "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets", "label": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets", "shape": "dot", "size": 10, "title": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension", "label": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension", "shape": "dot", "size": 35, "title": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models", "label": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models", "shape": "dot", "size": 20, "title": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Language Models as Knowledge Bases?", "label": "Language Models as Knowledge Bases?", "shape": "dot", "size": 25, "title": "Language Models as Knowledge Bases?"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Beyond Convexity: Stochastic Quasi-Convex Optimization", "label": "Beyond Convexity: Stochastic Quasi-Convex Optimization", "shape": "dot", "size": 10, "title": "Beyond Convexity: Stochastic Quasi-Convex Optimization"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Understanding Back-Translation at Scale", "label": "Understanding Back-Translation at Scale", "shape": "dot", "size": 15, "title": "Understanding Back-Translation at Scale"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Towards Deep Learning Models Resistant to Adversarial Attacks", "label": "Towards Deep Learning Models Resistant to Adversarial Attacks", "shape": "dot", "size": 55, "title": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "label": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "shape": "dot", "size": 20, "title": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Self-supervised Learning: Generative or Contrastive", "label": "Self-supervised Learning: Generative or Contrastive", "shape": "dot", "size": 10, "title": "Self-supervised Learning: Generative or Contrastive"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "label": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "shape": "dot", "size": 30, "title": "Learning Two Layer Rectified Neural Networks in Polynomial Time"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "How Context Affects Language Models\u0027 Factual Predictions", "label": "How Context Affects Language Models\u0027 Factual Predictions", "shape": "dot", "size": 15, "title": "How Context Affects Language Models\u0027 Factual Predictions"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Neural Kernels Without Tangents", "label": "Neural Kernels Without Tangents", "shape": "dot", "size": 15, "title": "Neural Kernels Without Tangents"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "label": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "shape": "dot", "size": 150, "title": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning One Convolutional Layer with Overlapping Patches", "label": "Learning One Convolutional Layer with Overlapping Patches", "shape": "dot", "size": 25, "title": "Learning One Convolutional Layer with Overlapping Patches"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A mean-field limit for certain deep neural networks", "label": "A mean-field limit for certain deep neural networks", "shape": "dot", "size": 35, "title": "A mean-field limit for certain deep neural networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Intracranial Error Detection via Deep Learning", "label": "Intracranial Error Detection via Deep Learning", "shape": "dot", "size": 10, "title": "Intracranial Error Detection via Deep Learning"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss", "label": "Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss", "shape": "dot", "size": 15, "title": "Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "When BERT Plays the Lottery, All Tickets Are Winning", "label": "When BERT Plays the Lottery, All Tickets Are Winning", "shape": "dot", "size": 10, "title": "When BERT Plays the Lottery, All Tickets Are Winning"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Adversarial Spheres", "label": "Adversarial Spheres", "shape": "dot", "size": 30, "title": "Adversarial Spheres"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs", "label": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs", "shape": "dot", "size": 15, "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "label": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "shape": "dot", "size": 55, "title": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Mean Field Analysis of Neural Networks: A Law of Large Numbers", "label": "Mean Field Analysis of Neural Networks: A Law of Large Numbers", "shape": "dot", "size": 75, "title": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Intriguing properties of neural networks", "label": "Intriguing properties of neural networks", "shape": "dot", "size": 50, "title": "Intriguing properties of neural networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks", "label": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks", "shape": "dot", "size": 45, "title": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "label": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "shape": "dot", "size": 15, "title": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning Transferable Architectures for Scalable Image Recognition", "label": "Learning Transferable Architectures for Scalable Image Recognition", "shape": "dot", "size": 10, "title": "Learning Transferable Architectures for Scalable Image Recognition"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "label": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "shape": "dot", "size": 30, "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples", "label": "A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples", "shape": "dot", "size": 15, "title": "A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis", "label": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis", "shape": "dot", "size": 80, "title": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Visualizing the Loss Landscape of Neural Nets", "label": "Visualizing the Loss Landscape of Neural Nets", "shape": "dot", "size": 10, "title": "Visualizing the Loss Landscape of Neural Nets"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Diverse Neural Network Learns True Target Functions", "label": "Diverse Neural Network Learns True Target Functions", "shape": "dot", "size": 45, "title": "Diverse Neural Network Learns True Target Functions"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds", "label": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds", "shape": "dot", "size": 35, "title": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Deep Learning Scaling is Predictable, Empirically", "label": "Deep Learning Scaling is Predictable, Empirically", "shape": "dot", "size": 15, "title": "Deep Learning Scaling is Predictable, Empirically"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Universal Language Model Fine-tuning for Text Classification", "label": "Universal Language Model Fine-tuning for Text Classification", "shape": "dot", "size": 40, "title": "Universal Language Model Fine-tuning for Text Classification"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks", "label": "High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks", "shape": "dot", "size": 10, "title": "High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "label": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "shape": "dot", "size": 95, "title": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics", "label": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics", "shape": "dot", "size": 15, "title": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience", "label": "Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience", "shape": "dot", "size": 10, "title": "Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks", "label": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks", "shape": "dot", "size": 50, "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems", "label": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems", "shape": "dot", "size": 10, "title": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Very Deep Transformers for Neural Machine Translation", "label": "Very Deep Transformers for Neural Machine Translation", "shape": "dot", "size": 10, "title": "Very Deep Transformers for Neural Machine Translation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Distilling the Knowledge in a Neural Network", "label": "Distilling the Knowledge in a Neural Network", "shape": "dot", "size": 60, "title": "Distilling the Knowledge in a Neural Network"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Mean Field View of the Landscape of Two-Layers Neural Networks", "label": "A Mean Field View of the Landscape of Two-Layers Neural Networks", "shape": "dot", "size": 45, "title": "A Mean Field View of the Landscape of Two-Layers Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Understanding the Difficulty of Training Transformers", "label": "Understanding the Difficulty of Training Transformers", "shape": "dot", "size": 15, "title": "Understanding the Difficulty of Training Transformers"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Provably Correct Algorithm for Deep Learning that Actually Works", "label": "A Provably Correct Algorithm for Deep Learning that Actually Works", "shape": "dot", "size": 10, "title": "A Provably Correct Algorithm for Deep Learning that Actually Works"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning Parities with Neural Networks", "label": "Learning Parities with Neural Networks", "shape": "dot", "size": 10, "title": "Learning Parities with Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Width Provably Matters in Optimization for Deep Linear Neural Networks", "label": "Width Provably Matters in Optimization for Deep Linear Neural Networks", "shape": "dot", "size": 15, "title": "Width Provably Matters in Optimization for Deep Linear Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Rademacher Complexity for Adversarially Robust Generalization", "label": "Rademacher Complexity for Adversarially Robust Generalization", "shape": "dot", "size": 15, "title": "Rademacher Complexity for Adversarially Robust Generalization"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "StereoSet: Measuring stereotypical bias in pretrained language models", "label": "StereoSet: Measuring stereotypical bias in pretrained language models", "shape": "dot", "size": 15, "title": "StereoSet: Measuring stereotypical bias in pretrained language models"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Mean Field Analysis of Deep Neural Networks", "label": "Mean Field Analysis of Deep Neural Networks", "shape": "dot", "size": 40, "title": "Mean Field Analysis of Deep Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning Two-layer Neural Networks with Symmetric Inputs", "label": "Learning Two-layer Neural Networks with Symmetric Inputs", "shape": "dot", "size": 25, "title": "Learning Two-layer Neural Networks with Symmetric Inputs"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$", "label": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$", "shape": "dot", "size": 10, "title": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks", "label": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks", "shape": "dot", "size": 15, "title": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Identity Matters in Deep Learning", "label": "Identity Matters in Deep Learning", "shape": "dot", "size": 40, "title": "Identity Matters in Deep Learning"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers", "label": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers", "shape": "dot", "size": 10, "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "label": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "shape": "dot", "size": 25, "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Simple Method for Commonsense Reasoning", "label": "A Simple Method for Commonsense Reasoning", "shape": "dot", "size": 60, "title": "A Simple Method for Commonsense Reasoning"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Cross-lingual Language Model Pretraining", "label": "Cross-lingual Language Model Pretraining", "shape": "dot", "size": 35, "title": "Cross-lingual Language Model Pretraining"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "label": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "shape": "dot", "size": 30, "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "How to Start Training: The Effect of Initialization and Architecture", "label": "How to Start Training: The Effect of Initialization and Architecture", "shape": "dot", "size": 35, "title": "How to Start Training: The Effect of Initialization and Architecture"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function", "label": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function", "shape": "dot", "size": 10, "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference", "label": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference", "shape": "dot", "size": 15, "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Technical report on Conversational Question Answering", "label": "Technical report on Conversational Question Answering", "shape": "dot", "size": 10, "title": "Technical report on Conversational Question Answering"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?", "label": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?", "shape": "dot", "size": 10, "title": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Understanding deep learning requires rethinking generalization", "label": "Understanding deep learning requires rethinking generalization", "shape": "dot", "size": 100, "title": "Understanding deep learning requires rethinking generalization"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Improving Neural Machine Translation Models with Monolingual Data", "label": "Improving Neural Machine Translation Models with Monolingual Data", "shape": "dot", "size": 20, "title": "Improving Neural Machine Translation Models with Monolingual Data"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions", "label": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions", "shape": "dot", "size": 10, "title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "label": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "shape": "dot", "size": 20, "title": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "label": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "shape": "dot", "size": 10, "title": "Tensor Programs II: Neural Tangent Kernel for Any Architecture"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Exploring the Limits of Language Modeling", "label": "Exploring the Limits of Language Modeling", "shape": "dot", "size": 45, "title": "Exploring the Limits of Language Modeling"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning to Optimize Neural Nets", "label": "Learning to Optimize Neural Nets", "shape": "dot", "size": 15, "title": "Learning to Optimize Neural Nets"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Recovery Guarantees for One-hidden-layer Neural Networks", "label": "Recovery Guarantees for One-hidden-layer Neural Networks", "shape": "dot", "size": 95, "title": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding", "label": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding", "shape": "dot", "size": 20, "title": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "label": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "shape": "dot", "size": 25, "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "label": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "shape": "dot", "size": 50, "title": "Learning One-hidden-layer ReLU Networks via Gradient Descent"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Tensor Programs III: Neural Matrix Laws", "label": "Tensor Programs III: Neural Matrix Laws", "shape": "dot", "size": 10, "title": "Tensor Programs III: Neural Matrix Laws"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "On the Power and Limitations of Random Features for Understanding Neural Networks", "label": "On the Power and Limitations of Random Features for Understanding Neural Networks", "shape": "dot", "size": 35, "title": "On the Power and Limitations of Random Features for Understanding Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "UnifiedQA: Crossing Format Boundaries With a Single QA System", "label": "UnifiedQA: Crossing Format Boundaries With a Single QA System", "shape": "dot", "size": 10, "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Fair is Better than Sensational:Man is to Doctor as Woman is to Doctor", "label": "Fair is Better than Sensational:Man is to Doctor as Woman is to Doctor", "shape": "dot", "size": 10, "title": "Fair is Better than Sensational:Man is to Doctor as Woman is to Doctor"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "label": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "shape": "dot", "size": 15, "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Dynamically Stable Infinite-Width Limits of Neural Classifiers", "label": "Dynamically Stable Infinite-Width Limits of Neural Classifiers", "shape": "dot", "size": 15, "title": "Dynamically Stable Infinite-Width Limits of Neural Classifiers"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "label": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "shape": "dot", "size": 10, "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "On the Selection of Initialization and Activation Function for Deep Neural Networks", "label": "On the Selection of Initialization and Activation Function for Deep Neural Networks", "shape": "dot", "size": 20, "title": "On the Selection of Initialization and Activation Function for Deep Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Deep Learning and Hierarchal Generative Models", "label": "Deep Learning and Hierarchal Generative Models", "shape": "dot", "size": 15, "title": "Deep Learning and Hierarchal Generative Models"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "On the infinite width limit of neural networks with a standard parameterization", "label": "On the infinite width limit of neural networks with a standard parameterization", "shape": "dot", "size": 10, "title": "On the infinite width limit of neural networks with a standard parameterization"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Benefits of depth in neural networks", "label": "Benefits of depth in neural networks", "shape": "dot", "size": 35, "title": "Benefits of depth in neural networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Generating Wikipedia by Summarizing Long Sequences", "label": "Generating Wikipedia by Summarizing Long Sequences", "shape": "dot", "size": 15, "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Story Ending Prediction by Transferable BERT", "label": "Story Ending Prediction by Transferable BERT", "shape": "dot", "size": 10, "title": "Story Ending Prediction by Transferable BERT"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "The large learning rate phase of deep learning: the catapult mechanism", "label": "The large learning rate phase of deep learning: the catapult mechanism", "shape": "dot", "size": 10, "title": "The large learning rate phase of deep learning: the catapult mechanism"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation", "label": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation", "shape": "dot", "size": 15, "title": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Adversarial Training for Large Neural Language Models", "label": "Adversarial Training for Large Neural Language Models", "shape": "dot", "size": 20, "title": "Adversarial Training for Large Neural Language Models"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "label": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "shape": "dot", "size": 55, "title": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "The Natural Language Decathlon: Multitask Learning as Question Answering", "label": "The Natural Language Decathlon: Multitask Learning as Question Answering", "shape": "dot", "size": 25, "title": "The Natural Language Decathlon: Multitask Learning as Question Answering"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes", "label": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes", "shape": "dot", "size": 10, "title": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference", "label": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference", "shape": "dot", "size": 15, "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks", "label": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks", "shape": "dot", "size": 65, "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "HellaSwag: Can a Machine Really Finish Your Sentence?", "label": "HellaSwag: Can a Machine Really Finish Your Sentence?", "shape": "dot", "size": 10, "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "The LAMBADA dataset: Word prediction requiring a broad discourse context", "label": "The LAMBADA dataset: Word prediction requiring a broad discourse context", "shape": "dot", "size": 15, "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "TTTTTackling WinoGrande Schemas", "label": "TTTTTackling WinoGrande Schemas", "shape": "dot", "size": 10, "title": "TTTTTackling WinoGrande Schemas"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "BERT Rediscovers the Classical NLP Pipeline", "label": "BERT Rediscovers the Classical NLP Pipeline", "shape": "dot", "size": 15, "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "label": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "shape": "dot", "size": 40, "title": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks", "label": "The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks", "shape": "dot", "size": 25, "title": "The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Probing Neural Network Comprehension of Natural Language Arguments", "label": "Probing Neural Network Comprehension of Natural Language Arguments", "shape": "dot", "size": 10, "title": "Probing Neural Network Comprehension of Natural Language Arguments"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Learning and Evaluating General Linguistic Intelligence", "label": "Learning and Evaluating General Linguistic Intelligence", "shape": "dot", "size": 15, "title": "Learning and Evaluating General Linguistic Intelligence"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Multi-Task Deep Neural Networks for Natural Language Understanding", "label": "Multi-Task Deep Neural Networks for Natural Language Understanding", "shape": "dot", "size": 40, "title": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Multilingual Denoising Pre-training for Neural Machine Translation", "label": "Multilingual Denoising Pre-training for Neural Machine Translation", "shape": "dot", "size": 10, "title": "Multilingual Denoising Pre-training for Neural Machine Translation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "TinyBERT: Distilling BERT for Natural Language Understanding", "label": "TinyBERT: Distilling BERT for Natural Language Understanding", "shape": "dot", "size": 25, "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks", "label": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks", "shape": "dot", "size": 45, "title": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Defending Against Neural Fake News", "label": "Defending Against Neural Fake News", "shape": "dot", "size": 20, "title": "Defending Against Neural Fake News"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Gaussian Error Linear Units (GELUs)", "label": "Gaussian Error Linear Units (GELUs)", "shape": "dot", "size": 40, "title": "Gaussian Error Linear Units (GELUs)"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Efficient Estimation of Word Representations in Vector Space", "label": "Efficient Estimation of Word Representations in Vector Space", "shape": "dot", "size": 30, "title": "Efficient Estimation of Word Representations in Vector Space"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "label": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "shape": "dot", "size": 20, "title": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Training Neural Networks with Local Error Signals", "label": "Training Neural Networks with Local Error Signals", "shape": "dot", "size": 10, "title": "Training Neural Networks with Local Error Signals"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "The Woman Worked as a Babysitter: On Biases in Language Generation", "label": "The Woman Worked as a Babysitter: On Biases in Language Generation", "shape": "dot", "size": 10, "title": "The Woman Worked as a Babysitter: On Biases in Language Generation"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Know What You Don\u0027t Know: Unanswerable Questions for SQuAD", "label": "Know What You Don\u0027t Know: Unanswerable Questions for SQuAD", "shape": "dot", "size": 25, "title": "Know What You Don\u0027t Know: Unanswerable Questions for SQuAD"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "label": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "shape": "dot", "size": 15, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Call for Clarity in Reporting BLEU Scores", "label": "A Call for Clarity in Reporting BLEU Scores", "shape": "dot", "size": 15, "title": "A Call for Clarity in Reporting BLEU Scores"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Automatic Detection of Generated Text is Easiest when Humans are Fooled", "label": "Automatic Detection of Generated Text is Easiest when Humans are Fooled", "shape": "dot", "size": 10, "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations", "label": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations", "shape": "dot", "size": 15, "title": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "label": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "shape": "dot", "size": 20, "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs", "label": "Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs", "shape": "dot", "size": 15, "title": "Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Gender Bias in Coreference Resolution", "label": "Gender Bias in Coreference Resolution", "shape": "dot", "size": 15, "title": "Gender Bias in Coreference Resolution"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories", "label": "A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories", "shape": "dot", "size": 15, "title": "A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "A Mean Field Theory of Batch Normalization", "label": "A Mean Field Theory of Batch Normalization", "shape": "dot", "size": 25, "title": "A Mean Field Theory of Batch Normalization"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality", "label": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality", "shape": "dot", "size": 10, "title": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "Kernel and Rich Regimes in Overparametrized Models", "label": "Kernel and Rich Regimes in Overparametrized Models", "shape": "dot", "size": 10, "title": "Kernel and Rich Regimes in Overparametrized Models"}, {"color": "#97c2fc", "font": {"color": "white"}, "id": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "label": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "shape": "dot", "size": 15, "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Feature Purification: How Adversarial Training Performs Robust Deep Learning"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Language Models are Few-Shot Learners"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "When Do Neural Networks Outperform Kernel Methods?"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "WARP: Word-level Adversarial ReProgramming"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Parameter-Efficient Transfer Learning for NLP"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Speeding up Convolutional Neural Networks with Low Rank Expansions"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Measuring the Intrinsic Dimension of Objective Landscapes"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "GPT Understands, Too"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Decoupled Weight Decay Regularization"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "DART: Open-Domain Structured Data Record to Text Generation"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "The E2E Dataset: New Challenges For End-to-End Generation"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Learning multiple visual domains with residual adapters"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Neural Network Acceptability Judgments"}, {"arrows": "to", "from": "LoRA: Low-Rank Adaptation of Large Language Models", "to": "Feature Learning in Infinite-Width Neural Networks"}, {"arrows": "to", "from": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "to": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation"}, {"arrows": "to", "from": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "to": "Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers"}, {"arrows": "to", "from": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "to": "Maxout Networks"}, {"arrows": "to", "from": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "to": "Improving neural networks by preventing co-adaptation of feature detectors"}, {"arrows": "to", "from": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "to": "DenseNet: Implementing Efficient ConvNet Descriptor Pyramids"}, {"arrows": "to", "from": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "to": "CNN Features off-the-shelf: an Astounding Baseline for Recognition"}, {"arrows": "to", "from": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "to": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"}, {"arrows": "to", "from": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "to": "DeepPose: Human Pose Estimation via Deep Neural Networks"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "Better Fine-Tuning by Reducing Representational Collapse"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "Stronger generalization bounds for deep nets via a compression approach"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "What Does BERT Look At? An Analysis of BERT\u0027s Attention"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "Evaluating Lottery Tickets Under Distributional Shifts"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "Parameter-Efficient Transfer Learning for NLP"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "Measuring the Intrinsic Dimension of Objective Landscapes"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "When BERT Plays the Lottery, All Tickets Are Winning"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "to": "Character-level Convolutional Networks for Text Classification"}, {"arrows": "to", "from": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "to": "Character-Level Language Modeling with Deeper Self-Attention"}, {"arrows": "to", "from": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "to": "MaskGAN: Better Text Generation via Filling in the______"}, {"arrows": "to", "from": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "to": "U-Net: Machine Reading Comprehension with Unanswerable Questions"}, {"arrows": "to", "from": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "to": "Neural Network Acceptability Judgments"}, {"arrows": "to", "from": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "to": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"arrows": "to", "from": "Learning multiple visual domains with residual adapters", "to": "Universal representations:The missing link between faces, text, planktons, and cat breeds"}, {"arrows": "to", "from": "Learning multiple visual domains with residual adapters", "to": "Incremental Learning Through Deep Adaptation"}, {"arrows": "to", "from": "Learning multiple visual domains with residual adapters", "to": "Progressive Neural Networks"}, {"arrows": "to", "from": "Learning multiple visual domains with residual adapters", "to": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"}, {"arrows": "to", "from": "Learning multiple visual domains with residual adapters", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "DART: Open-Domain Structured Data Record to Text Generation", "to": "Transforming Question Answering Datasets Into Natural Language Inference Datasets"}, {"arrows": "to", "from": "DART: Open-Domain Structured Data Record to Text Generation", "to": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity"}, {"arrows": "to", "from": "DART: Open-Domain Structured Data Record to Text Generation", "to": "Text-to-Text Pre-Training for Data-to-Text Tasks"}, {"arrows": "to", "from": "Parameter-Efficient Transfer Learning for NLP", "to": "Universal Sentence Encoder"}, {"arrows": "to", "from": "Parameter-Efficient Transfer Learning for NLP", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Parameter-Efficient Transfer Learning for NLP", "to": "In Defense of the Triplet Loss for Person Re-Identification"}, {"arrows": "to", "from": "Parameter-Efficient Transfer Learning for NLP", "to": "What makes ImageNet good for transfer learning?"}, {"arrows": "to", "from": "Parameter-Efficient Transfer Learning for NLP", "to": "Do Better ImageNet Models Transfer Better?"}, {"arrows": "to", "from": "Parameter-Efficient Transfer Learning for NLP", "to": "Progressive Neural Networks"}, {"arrows": "to", "from": "Parameter-Efficient Transfer Learning for NLP", "to": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Stronger generalization bounds for deep nets via a compression approach"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Reconciling modern machine learning practice and the bias-variance trade-off"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Two models of double descent for weak features"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Linearized two-layers neural networks in high dimension"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Learning One Convolutional Layer with Overlapping Patches"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Size-Independent Sample Complexity of Neural Networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Train faster, generalize better: Stability of stochastic gradient descent"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Gradient descent aligns the layers of deep linear networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Risk and parameter convergence of logistic regression"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Generalization bounds for deep convolutional neural networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "A Mean Field View of the Landscape of Two-Layers Neural Networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "On the Power and Limitations of Random Features for Understanding Neural Networks"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Rademacher Complexity for Adversarially Robust Generalization"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$"}, {"arrows": "to", "from": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Neural Network Acceptability Judgments", "to": "RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency"}, {"arrows": "to", "from": "The E2E Dataset: New Challenges For End-to-End Generation", "to": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings"}, {"arrows": "to", "from": "The E2E Dataset: New Challenges For End-to-End Generation", "to": "A Context-aware Natural Language Generator for Dialogue Systems"}, {"arrows": "to", "from": "The E2E Dataset: New Challenges For End-to-End Generation", "to": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment"}, {"arrows": "to", "from": "The E2E Dataset: New Challenges For End-to-End Generation", "to": "Crowd-sourcing NLG Data: Pictures Elicit Better Data"}, {"arrows": "to", "from": "The E2E Dataset: New Challenges For End-to-End Generation", "to": "Multi-domain Neural Network Language Generation for Spoken Dialogue Systems"}, {"arrows": "to", "from": "When Do Neural Networks Outperform Kernel Methods?", "to": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning"}, {"arrows": "to", "from": "When Do Neural Networks Outperform Kernel Methods?", "to": "Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss"}, {"arrows": "to", "from": "When Do Neural Networks Outperform Kernel Methods?", "to": "Linearized two-layers neural networks in high dimension"}, {"arrows": "to", "from": "When Do Neural Networks Outperform Kernel Methods?", "to": "Disentangling feature and lazy training in deep neural networks"}, {"arrows": "to", "from": "When Do Neural Networks Outperform Kernel Methods?", "to": "Enhanced Convolutional Neural Tangent Kernels"}, {"arrows": "to", "from": "When Do Neural Networks Outperform Kernel Methods?", "to": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks"}, {"arrows": "to", "from": "When Do Neural Networks Outperform Kernel Methods?", "to": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks"}, {"arrows": "to", "from": "When Do Neural Networks Outperform Kernel Methods?", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "When Do Neural Networks Outperform Kernel Methods?", "to": "Neural Kernels Without Tangents"}, {"arrows": "to", "from": "When Do Neural Networks Outperform Kernel Methods?", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "When Do Neural Networks Outperform Kernel Methods?", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Beyond Convexity: Stochastic Quasi-Convex Optimization"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "AutoAugment: Learning Augmentation Policies from Data"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Sharp Minima Can Generalize For Deep Nets"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Shake-Shake regularization"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Snapshot Ensembles: Train 1, get M for free"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Visualizing the Loss Landscape of Neural Nets"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Cyclical Learning Rates for Training Neural Networks"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Intracranial Error Detection via Deep Learning"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "SFace: An Efficient Network for Face Detection in Large Scale Variations"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "The Marginal Value of Adaptive Gradient Methods in Machine Learning"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Three Mechanisms of Weight Decay Regularization"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Normalized Direction-preserving Adam"}, {"arrows": "to", "from": "Decoupled Weight Decay Regularization", "to": "Learning Transferable Architectures for Scalable Image Recognition"}, {"arrows": "to", "from": "WARP: Word-level Adversarial ReProgramming", "to": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"arrows": "to", "from": "WARP: Word-level Adversarial ReProgramming", "to": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Learning Two Layer Rectified Neural Networks in Polynomial Time"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Theoretical properties of the global optimizer of two layer neural network"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Adversarial examples from computational constraints"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Adversarial Examples Are a Natural Consequence of Test Error in Noise"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Linearized two-layers neural networks in high dimension"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Adversarial Spheres"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Explaining and Harnessing Adversarial Examples"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Countering Adversarial Images using Input Transformations"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Finite Depth and Width Corrections to the Neural Tangent Kernel"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Precise Tradeoffs in Adversarial Training for Linear Regression"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Making Method of Moments Great Again? -- How can GANs learn distributions"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Adversarial Training Can Hurt Generalization"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Are adversarial examples inevitable?"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Revisiting Adversarial Risk"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Intriguing properties of neural networks"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Diverse Neural Network Learns True Target Functions"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Learning One-hidden-layer ReLU Networks via Gradient Descent"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Feature Purification: How Adversarial Training Performs Robust Deep Learning", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Why bigger is not always better: on finite and infinite neural networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Deep kernel processes"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "A mean-field limit for certain deep neural networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Language Models are Few-Shot Learners"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Language Models are Few-Shot Learners"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Dynamically Stable Infinite-Width Limits of Neural Classifiers"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "How to Start Training: The Effect of Initialization and Architecture"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "On the Selection of Initialization and Activation Function for Deep Neural Networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Gaussian Error Linear Units (GELUs)"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "The large learning rate phase of deep learning: the catapult mechanism"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Mean Field Analysis of Deep Neural Networks"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "On the infinite width limit of neural networks with a standard parameterization"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Kernel and Rich Regimes in Overparametrized Models"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Tensor Programs II: Neural Tangent Kernel for Any Architecture"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Tensor Programs III: Neural Matrix Laws"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "A Mean Field Theory of Batch Normalization"}, {"arrows": "to", "from": "Feature Learning in Infinite-Width Neural Networks", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "to": "GeDi: Generative Discriminator Guided Sequence Generation"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "Language Models are Few-Shot Learners"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "What Does BERT Look At? An Analysis of BERT\u0027s Attention"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "Making Pre-trained Language Models Better Few-shot Learners"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "What Makes Good In-Context Examples for GPT-$3$?"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "Self-supervised Learning: Generative or Contrastive"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "How Context Affects Language Models\u0027 Factual Predictions"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "Language Models as Knowledge Bases?"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "It\u0027s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "A Multiscale Visualization of Attention in the Transformer Model"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "Language Models are Open Knowledge Graphs"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"}, {"arrows": "to", "from": "GPT Understands, Too", "to": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"}, {"arrows": "to", "from": "Measuring the Intrinsic Dimension of Objective Landscapes", "to": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u003c0.5MB model size"}, {"arrows": "to", "from": "Measuring the Intrinsic Dimension of Objective Landscapes", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Learning Two Layer Rectified Neural Networks in Polynomial Time"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Theoretical properties of the global optimizer of two layer neural network"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Learning Parities with Neural Networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Width Provably Matters in Optimization for Deep Linear Neural Networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Learning Two-layer Neural Networks with Symmetric Inputs"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Linearized two-layers neural networks in high dimension"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Finite Depth and Width Corrections to the Neural Tangent Kernel"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Identity Matters in Deep Learning"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Distilling the Knowledge in a Neural Network"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Making Method of Moments Great Again? -- How can GANs learn distributions"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Enhanced Convolutional Neural Tangent Kernels"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Understanding the Difficulty of Training Transformers"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Very Deep Transformers for Neural Machine Translation"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "A Provably Correct Algorithm for Deep Learning that Actually Works"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Deep Learning and Hierarchal Generative Models"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Training Neural Networks with Local Error Signals"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Neural Kernels Without Tangents"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Benefits of depth in neural networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "BERT Rediscovers the Classical NLP Pipeline"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Diverse Neural Network Learns True Target Functions"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "On the Power and Limitations of Random Features for Understanding Neural Networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Learning One-hidden-layer ReLU Networks via Gradient Descent"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Maxout Networks", "to": "Improving neural networks by preventing co-adaptation of feature detectors"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Experience Grounds Language"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "UNITER: UNiversal Image-TExt Representation Learning"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Understanding Back-Translation at Scale"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Annotation Artifacts in Natural Language Inference Data"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "GLTR: Statistical Detection and Visualization of Generated Text"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Meta-Learning for Low-Resource Neural Machine Translation"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Pretrained Transformers Improve Out-of-Distribution Robustness"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Deep Learning Scaling is Predictable, Empirically"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Universal Language Model Fine-tuning for Text Classification"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Distilling the Knowledge in a Neural Network"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Automatic Detection of Generated Text is Easiest when Humans are Fooled"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Exploring the Limits of Language Modeling"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Technical report on Conversational Question Answering"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "UnifiedQA: Crossing Format Boundaries With a Single QA System"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Cross-lingual Language Model Pretraining"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Adversarial Training for Large Neural Language Models"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Story Ending Prediction by Transferable BERT"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Multilingual Denoising Pre-training for Neural Machine Translation"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Decoupled Weight Decay Regularization"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Learning to Optimize Neural Nets"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Generating Wikipedia by Summarizing Long Sequences"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "TTTTTackling WinoGrande Schemas"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Efficient Estimation of Word Representations in Vector Space"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "The Natural Language Decathlon: Multitask Learning as Question Answering"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "StereoSet: Measuring stereotypical bias in pretrained language models"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Probing Neural Network Comprehension of Natural Language Arguments"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Fair is Better than Sensational:Man is to Doctor as Woman is to Doctor"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "A Call for Clarity in Reporting BLEU Scores"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Know What You Don\u0027t Know: Unanswerable Questions for SQuAD"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Gender Bias in Coreference Resolution"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "The Woman Worked as a Babysitter: On Biases in Language Generation"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Improving Neural Machine Translation Models with Monolingual Data"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "A Simple Method for Commonsense Reasoning"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Learning and Evaluating General Linguistic Intelligence"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "Defending Against Neural Fake News"}, {"arrows": "to", "from": "Language Models are Few-Shot Learners", "to": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"}, {"arrows": "to", "from": "Better Fine-Tuning by Reducing Representational Collapse", "to": "What Does BERT Look At? An Analysis of BERT\u0027s Attention"}, {"arrows": "to", "from": "Better Fine-Tuning by Reducing Representational Collapse", "to": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"arrows": "to", "from": "Better Fine-Tuning by Reducing Representational Collapse", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Better Fine-Tuning by Reducing Representational Collapse", "to": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"arrows": "to", "from": "Better Fine-Tuning by Reducing Representational Collapse", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Better Fine-Tuning by Reducing Representational Collapse", "to": "Neural Network Acceptability Judgments"}, {"arrows": "to", "from": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "to": "Improving neural networks by preventing co-adaptation of feature detectors"}, {"arrows": "to", "from": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "to": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"arrows": "to", "from": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "to": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"arrows": "to", "from": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "to": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"arrows": "to", "from": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "to": "Neural Network Acceptability Judgments"}, {"arrows": "to", "from": "Unsupervised Cross-lingual Representation Learning at Scale", "to": "Exploring the Limits of Language Modeling"}, {"arrows": "to", "from": "Unsupervised Cross-lingual Representation Learning at Scale", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Unsupervised Cross-lingual Representation Learning at Scale", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "Incremental Learning Through Deep Adaptation", "to": "Learning multiple visual domains with residual adapters"}, {"arrows": "to", "from": "Incremental Learning Through Deep Adaptation", "to": "Progressive Neural Networks"}, {"arrows": "to", "from": "Incremental Learning Through Deep Adaptation", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Stronger generalization bounds for deep nets via a compression approach", "to": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys"}, {"arrows": "to", "from": "Stronger generalization bounds for deep nets via a compression approach", "to": "Sharp Minima Can Generalize For Deep Nets"}, {"arrows": "to", "from": "Stronger generalization bounds for deep nets via a compression approach", "to": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data"}, {"arrows": "to", "from": "Stronger generalization bounds for deep nets via a compression approach", "to": "Size-Independent Sample Complexity of Neural Networks"}, {"arrows": "to", "from": "Stronger generalization bounds for deep nets via a compression approach", "to": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"arrows": "to", "from": "Stronger generalization bounds for deep nets via a compression approach", "to": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"}, {"arrows": "to", "from": "Transforming Question Answering Datasets Into Natural Language Inference Datasets", "to": "Annotation Artifacts in Natural Language Inference Data"}, {"arrows": "to", "from": "Transforming Question Answering Datasets Into Natural Language Inference Datasets", "to": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"arrows": "to", "from": "What Does BERT Look At? An Analysis of BERT\u0027s Attention", "to": "BERT Rediscovers the Classical NLP Pipeline"}, {"arrows": "to", "from": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "to": "Gaussian Error Linear Units (GELUs)"}, {"arrows": "to", "from": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "to": "Cross-lingual Language Model Pretraining"}, {"arrows": "to", "from": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "to": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"arrows": "to", "from": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "to": "Efficient Estimation of Word Representations in Vector Space"}, {"arrows": "to", "from": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings", "to": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment"}, {"arrows": "to", "from": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "to": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"arrows": "to", "from": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "to": "A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories"}, {"arrows": "to", "from": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "to": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"arrows": "to", "from": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "to": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"arrows": "to", "from": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "to": "Gaussian Error Linear Units (GELUs)"}, {"arrows": "to", "from": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "to": "Universal Language Model Fine-tuning for Text Classification"}, {"arrows": "to", "from": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "to": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"arrows": "to", "from": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "to": "Cross-lingual Language Model Pretraining"}, {"arrows": "to", "from": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "to": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding"}, {"arrows": "to", "from": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "to": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"arrows": "to", "from": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "to": "A Simple Method for Commonsense Reasoning"}, {"arrows": "to", "from": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "to": "Defending Against Neural Fake News"}, {"arrows": "to", "from": "Universal representations:The missing link between faces, text, planktons, and cat breeds", "to": "Distilling the Knowledge in a Neural Network"}, {"arrows": "to", "from": "Text-to-Text Pre-Training for Data-to-Text Tasks", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Text-to-Text Pre-Training for Data-to-Text Tasks", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "AutoAugment: Learning Augmentation Policies from Data", "to": "Shake-Shake regularization"}, {"arrows": "to", "from": "AutoAugment: Learning Augmentation Policies from Data", "to": "Do Better ImageNet Models Transfer Better?"}, {"arrows": "to", "from": "AutoAugment: Learning Augmentation Policies from Data", "to": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Stronger generalization bounds for deep nets via a compression approach"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Size-Independent Sample Complexity of Neural Networks"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Identity Matters in Deep Learning"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Train faster, generalize better: Stability of stochastic gradient descent"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Gradient descent aligns the layers of deep linear networks"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "A Mean Field View of the Landscape of Two-Layers Neural Networks"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Learning One-hidden-layer ReLU Networks via Gradient Descent"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "On Lazy Training in Differentiable Programming", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity", "to": "The E2E Dataset: New Challenges For End-to-End Generation"}, {"arrows": "to", "from": "Disentangling feature and lazy training in deep neural networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Disentangling feature and lazy training in deep neural networks", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Disentangling feature and lazy training in deep neural networks", "to": "A Mean Field View of the Landscape of Two-Layers Neural Networks"}, {"arrows": "to", "from": "Disentangling feature and lazy training in deep neural networks", "to": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"arrows": "to", "from": "Disentangling feature and lazy training in deep neural networks", "to": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks"}, {"arrows": "to", "from": "Disentangling feature and lazy training in deep neural networks", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Disentangling feature and lazy training in deep neural networks", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "Disentangling feature and lazy training in deep neural networks", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits", "to": "Reconciling modern machine learning practice and the bias-variance trade-off"}, {"arrows": "to", "from": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits", "to": "Linearized two-layers neural networks in high dimension"}, {"arrows": "to", "from": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits", "to": "A Mean Field View of the Landscape of Two-Layers Neural Networks"}, {"arrows": "to", "from": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "A Convergence Theory for Deep Learning via Over-Parameterization", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "A Convergence Theory for Deep Learning via Over-Parameterization", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "A Convergence Theory for Deep Learning via Over-Parameterization", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "A Convergence Theory for Deep Learning via Over-Parameterization", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "A Convergence Theory for Deep Learning via Over-Parameterization", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport", "to": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Feature Purification: How Adversarial Training Performs Robust Deep Learning"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Stronger generalization bounds for deep nets via a compression approach"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Learning Two Layer Rectified Neural Networks in Polynomial Time"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Theoretical properties of the global optimizer of two layer neural network"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Diverse Neural Network Learns True Target Functions"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "to": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"}, {"arrows": "to", "from": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "to": "Gaussian Error Linear Units (GELUs)"}, {"arrows": "to", "from": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "to": "Universal Language Model Fine-tuning for Text Classification"}, {"arrows": "to", "from": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "to": "Neural Network Acceptability Judgments"}, {"arrows": "to", "from": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "to": "Language Models are Few-Shot Learners"}, {"arrows": "to", "from": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "to": "Adversarial Training for Large Neural Language Models"}, {"arrows": "to", "from": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "to": "A Simple Method for Commonsense Reasoning"}, {"arrows": "to", "from": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "to": "Neural Network Acceptability Judgments"}, {"arrows": "to", "from": "RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency", "to": "Gender Bias in Coreference Resolution"}, {"arrows": "to", "from": "GeDi: Generative Discriminator Guided Sequence Generation", "to": "Language Models are Few-Shot Learners"}, {"arrows": "to", "from": "GeDi: Generative Discriminator Guided Sequence Generation", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "GeDi: Generative Discriminator Guided Sequence Generation", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "What Makes Good In-Context Examples for GPT-$3$?", "to": "Language Models are Few-Shot Learners"}, {"arrows": "to", "from": "What Makes Good In-Context Examples for GPT-$3$?", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "What Makes Good In-Context Examples for GPT-$3$?", "to": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"arrows": "to", "from": "What Makes Good In-Context Examples for GPT-$3$?", "to": "Cross-lingual Language Model Pretraining"}, {"arrows": "to", "from": "What Makes Good In-Context Examples for GPT-$3$?", "to": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"arrows": "to", "from": "What Makes Good In-Context Examples for GPT-$3$?", "to": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"arrows": "to", "from": "What Makes Good In-Context Examples for GPT-$3$?", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "What Makes Good In-Context Examples for GPT-$3$?", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Understanding Back-Translation at Scale"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Deep Learning Scaling is Predictable, Empirically"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Distilling the Knowledge in a Neural Network"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Parameter-Efficient Transfer Learning for NLP"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Universal Language Model Fine-tuning for Text Classification"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "What makes ImageNet good for transfer learning?"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Exploring the Limits of Language Modeling"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Do Better ImageNet Models Transfer Better?"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Cross-lingual Language Model Pretraining"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Generating Wikipedia by Summarizing Long Sequences"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "The Natural Language Decathlon: Multitask Learning as Question Answering"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Efficient Estimation of Word Representations in Vector Space"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "A Call for Clarity in Reporting BLEU Scores"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "A Simple Method for Commonsense Reasoning"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Neural Network Acceptability Judgments"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "Defending Against Neural Fake News"}, {"arrows": "to", "from": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "to": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"}, {"arrows": "to", "from": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u003c0.5MB model size", "to": "DenseNet: Implementing Efficient ConvNet Descriptor Pyramids"}, {"arrows": "to", "from": "Generalization bounds for deep convolutional neural networks", "to": "Stronger generalization bounds for deep nets via a compression approach"}, {"arrows": "to", "from": "Generalization bounds for deep convolutional neural networks", "to": "Size-Independent Sample Complexity of Neural Networks"}, {"arrows": "to", "from": "Generalization bounds for deep convolutional neural networks", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Generalization bounds for deep convolutional neural networks", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "to": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"arrows": "to", "from": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "to": "Identity Matters in Deep Learning"}, {"arrows": "to", "from": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Making Pre-trained Language Models Better Few-shot Learners", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Making Pre-trained Language Models Better Few-shot Learners", "to": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"arrows": "to", "from": "Making Pre-trained Language Models Better Few-shot Learners", "to": "A Simple Method for Commonsense Reasoning"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "Reconciling modern machine learning practice and the bias-variance trade-off"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "Two models of double descent for weak features"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "Disentangling feature and lazy training in deep neural networks"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "On the Power and Limitations of Random Features for Understanding Neural Networks"}, {"arrows": "to", "from": "Linearized two-layers neural networks in high dimension", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "to": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation"}, {"arrows": "to", "from": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "to": "StereoSet: Measuring stereotypical bias in pretrained language models"}, {"arrows": "to", "from": "Do Better ImageNet Models Transfer Better?", "to": "AutoAugment: Learning Augmentation Policies from Data"}, {"arrows": "to", "from": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "to": "Language Models are Few-Shot Learners"}, {"arrows": "to", "from": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "to": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference"}, {"arrows": "to", "from": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "to": "A Simple Method for Commonsense Reasoning"}, {"arrows": "to", "from": "Normalized Direction-preserving Adam", "to": "Decoupled Weight Decay Regularization"}, {"arrows": "to", "from": "Normalized Direction-preserving Adam", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "to": "Train faster, generalize better: Stability of stochastic gradient descent"}, {"arrows": "to", "from": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks", "to": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"arrows": "to", "from": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks", "to": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"arrows": "to", "from": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "What makes ImageNet good for transfer learning?", "to": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"}, {"arrows": "to", "from": "SGDR: Stochastic Gradient Descent with Warm Restarts", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "SGDR: Stochastic Gradient Descent with Warm Restarts", "to": "Cyclical Learning Rates for Training Neural Networks"}, {"arrows": "to", "from": "SGDR: Stochastic Gradient Descent with Warm Restarts", "to": "Cyclical Learning Rates for Training Neural Networks"}, {"arrows": "to", "from": "SGDR: Stochastic Gradient Descent with Warm Restarts", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "to": "Reconciling modern machine learning practice and the bias-variance trade-off"}, {"arrows": "to", "from": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "to": "Two models of double descent for weak features"}, {"arrows": "to", "from": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "to": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks"}, {"arrows": "to", "from": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "to": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"arrows": "to", "from": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "to": "A Mean Field View of the Landscape of Two-Layers Neural Networks"}, {"arrows": "to", "from": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "Identity Matters in Deep Learning"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "Learning One-hidden-layer ReLU Networks via Gradient Descent"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Gradient Descent Finds Global Minima of Deep Neural Networks", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Snapshot Ensembles: Train 1, get M for free", "to": "Distilling the Knowledge in a Neural Network"}, {"arrows": "to", "from": "Snapshot Ensembles: Train 1, get M for free", "to": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"arrows": "to", "from": "Snapshot Ensembles: Train 1, get M for free", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Snapshot Ensembles: Train 1, get M for free", "to": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"arrows": "to", "from": "Snapshot Ensembles: Train 1, get M for free", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs", "to": "Identity Matters in Deep Learning"}, {"arrows": "to", "from": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Size-Independent Sample Complexity of Neural Networks", "to": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"}, {"arrows": "to", "from": "Size-Independent Sample Complexity of Neural Networks", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Character-level Convolutional Networks for Text Classification", "to": "Improving neural networks by preventing co-adaptation of feature detectors"}, {"arrows": "to", "from": "Three Mechanisms of Weight Decay Regularization", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "Three Mechanisms of Weight Decay Regularization", "to": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"arrows": "to", "from": "Three Mechanisms of Weight Decay Regularization", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Three Mechanisms of Weight Decay Regularization", "to": "Decoupled Weight Decay Regularization"}, {"arrows": "to", "from": "Three Mechanisms of Weight Decay Regularization", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "Sharp Minima Can Generalize For Deep Nets", "to": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys"}, {"arrows": "to", "from": "Sharp Minima Can Generalize For Deep Nets", "to": "Explaining and Harnessing Adversarial Examples"}, {"arrows": "to", "from": "Sharp Minima Can Generalize For Deep Nets", "to": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"arrows": "to", "from": "Sharp Minima Can Generalize For Deep Nets", "to": "Intriguing properties of neural networks"}, {"arrows": "to", "from": "Sharp Minima Can Generalize For Deep Nets", "to": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"arrows": "to", "from": "Sharp Minima Can Generalize For Deep Nets", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "to": "Making Pre-trained Language Models Better Few-shot Learners"}, {"arrows": "to", "from": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "to": "It\u0027s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"}, {"arrows": "to", "from": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "to": "A Simple Method for Commonsense Reasoning"}, {"arrows": "to", "from": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "to": "Learning and Evaluating General Linguistic Intelligence"}, {"arrows": "to", "from": "UNITER: UNiversal Image-TExt Representation Learning", "to": "Distilling the Knowledge in a Neural Network"}, {"arrows": "to", "from": "UNITER: UNiversal Image-TExt Representation Learning", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "UNITER: UNiversal Image-TExt Representation Learning", "to": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"arrows": "to", "from": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality", "to": "Explaining and Harnessing Adversarial Examples"}, {"arrows": "to", "from": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality", "to": "Intriguing properties of neural networks"}, {"arrows": "to", "from": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality", "to": "A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples"}, {"arrows": "to", "from": "GLTR: Statistical Detection and Visualization of Generated Text", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "to": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys"}, {"arrows": "to", "from": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "to": "Explaining and Harnessing Adversarial Examples"}, {"arrows": "to", "from": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "to": "Train faster, generalize better: Stability of stochastic gradient descent"}, {"arrows": "to", "from": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "On Exact Computation with an Infinitely Wide Neural Net", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "On Exact Computation with an Infinitely Wide Neural Net", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "On Exact Computation with an Infinitely Wide Neural Net", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "On Exact Computation with an Infinitely Wide Neural Net", "to": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "On Exact Computation with an Infinitely Wide Neural Net", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "On Exact Computation with an Infinitely Wide Neural Net", "to": "Width Provably Matters in Optimization for Deep Linear Neural Networks"}, {"arrows": "to", "from": "On Exact Computation with an Infinitely Wide Neural Net", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "On Exact Computation with an Infinitely Wide Neural Net", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "On Exact Computation with an Infinitely Wide Neural Net", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "On Exact Computation with an Infinitely Wide Neural Net", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Feature Purification: How Adversarial Training Performs Robust Deep Learning"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Learning Two Layer Rectified Neural Networks in Polynomial Time"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Theoretical properties of the global optimizer of two layer neural network"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Linearized two-layers neural networks in high dimension"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Finite Depth and Width Corrections to the Neural Tangent Kernel"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Diverse Neural Network Learns True Target Functions"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Learning One-hidden-layer ReLU Networks via Gradient Descent"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Making Method of Moments Great Again? -- How can GANs learn distributions", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data", "to": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys"}, {"arrows": "to", "from": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "SFace: An Efficient Network for Face Detection in Large Scale Variations", "to": "Decoupled Weight Decay Regularization"}, {"arrows": "to", "from": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "to": "Maxout Networks"}, {"arrows": "to", "from": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "to": "Train faster, generalize better: Stability of stochastic gradient descent"}, {"arrows": "to", "from": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "to": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"}, {"arrows": "to", "from": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "to": "AutoAugment: Learning Augmentation Policies from Data"}, {"arrows": "to", "from": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "to": "Adversarial Spheres"}, {"arrows": "to", "from": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "to": "Countering Adversarial Images using Input Transformations"}, {"arrows": "to", "from": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "to": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"arrows": "to", "from": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "to": "Distilling the Knowledge in a Neural Network"}, {"arrows": "to", "from": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "to": "Improving neural networks by preventing co-adaptation of feature detectors"}, {"arrows": "to", "from": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "to": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u003c0.5MB model size"}, {"arrows": "to", "from": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Why bigger is not always better: on finite and infinite neural networks", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Why bigger is not always better: on finite and infinite neural networks", "to": "What makes ImageNet good for transfer learning?"}, {"arrows": "to", "from": "Why bigger is not always better: on finite and infinite neural networks", "to": "Enhanced Convolutional Neural Tangent Kernels"}, {"arrows": "to", "from": "Why bigger is not always better: on finite and infinite neural networks", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "to": "Transforming Question Answering Datasets Into Natural Language Inference Datasets"}, {"arrows": "to", "from": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "to": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"}, {"arrows": "to", "from": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "to": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"arrows": "to", "from": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "to": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"}, {"arrows": "to", "from": "Enhanced Convolutional Neural Tangent Kernels", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Enhanced Convolutional Neural Tangent Kernels", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Enhanced Convolutional Neural Tangent Kernels", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Enhanced Convolutional Neural Tangent Kernels", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "Enhanced Convolutional Neural Tangent Kernels", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Enhanced Convolutional Neural Tangent Kernels", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Cyclical Learning Rates for Training Neural Networks", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "Cyclical Learning Rates for Training Neural Networks", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Cyclical Learning Rates for Training Neural Networks", "to": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"arrows": "to", "from": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", "to": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"}, {"arrows": "to", "from": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", "to": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks"}, {"arrows": "to", "from": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Adam: A Method for Stochastic Optimization", "to": "Improving neural networks by preventing co-adaptation of feature detectors"}, {"arrows": "to", "from": "Deep kernel processes", "to": "Why bigger is not always better: on finite and infinite neural networks"}, {"arrows": "to", "from": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "Learning One-hidden-layer Neural Networks with Landscape Design", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Learning One-hidden-layer Neural Networks with Landscape Design", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Learning One-hidden-layer Neural Networks with Landscape Design", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Learning One-hidden-layer Neural Networks with Landscape Design", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "to": "Character-Level Language Modeling with Deeper Self-Attention"}, {"arrows": "to", "from": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "to": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"}, {"arrows": "to", "from": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "to": "MaskGAN: Better Text Generation via Filling in the______"}, {"arrows": "to", "from": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "to": "Universal Language Model Fine-tuning for Text Classification"}, {"arrows": "to", "from": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "to": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"arrows": "to", "from": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "to": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"arrows": "to", "from": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "to": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"arrows": "to", "from": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "to": "Know What You Don\u0027t Know: Unanswerable Questions for SQuAD"}, {"arrows": "to", "from": "REALM: Retrieval-Augmented Language Model Pre-Training", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "REALM: Retrieval-Augmented Language Model Pre-Training", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "REALM: Retrieval-Augmented Language Model Pre-Training", "to": "Efficient Estimation of Word Representations in Vector Space"}, {"arrows": "to", "from": "REALM: Retrieval-Augmented Language Model Pre-Training", "to": "Language Models as Knowledge Bases?"}, {"arrows": "to", "from": "REALM: Retrieval-Augmented Language Model Pre-Training", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "REALM: Retrieval-Augmented Language Model Pre-Training", "to": "Know What You Don\u0027t Know: Unanswerable Questions for SQuAD"}, {"arrows": "to", "from": "Are adversarial examples inevitable?", "to": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"}, {"arrows": "to", "from": "Are adversarial examples inevitable?", "to": "Adversarial Spheres"}, {"arrows": "to", "from": "Are adversarial examples inevitable?", "to": "Explaining and Harnessing Adversarial Examples"}, {"arrows": "to", "from": "Are adversarial examples inevitable?", "to": "Countering Adversarial Images using Input Transformations"}, {"arrows": "to", "from": "Are adversarial examples inevitable?", "to": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality"}, {"arrows": "to", "from": "Are adversarial examples inevitable?", "to": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"arrows": "to", "from": "Are adversarial examples inevitable?", "to": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models"}, {"arrows": "to", "from": "Are adversarial examples inevitable?", "to": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples"}, {"arrows": "to", "from": "Are adversarial examples inevitable?", "to": "Intriguing properties of neural networks"}, {"arrows": "to", "from": "Adversarial Training Can Hurt Generalization", "to": "Rademacher Complexity for Adversarially Robust Generalization"}, {"arrows": "to", "from": "Precise Tradeoffs in Adversarial Training for Linear Regression", "to": "Reconciling modern machine learning practice and the bias-variance trade-off"}, {"arrows": "to", "from": "Precise Tradeoffs in Adversarial Training for Linear Regression", "to": "Adversarial Spheres"}, {"arrows": "to", "from": "Precise Tradeoffs in Adversarial Training for Linear Regression", "to": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"arrows": "to", "from": "Precise Tradeoffs in Adversarial Training for Linear Regression", "to": "Adversarial Training Can Hurt Generalization"}, {"arrows": "to", "from": "No bad local minima: Data independent training error guarantees for multilayer neural networks", "to": "Train faster, generalize better: Stability of stochastic gradient descent"}, {"arrows": "to", "from": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples", "to": "Explaining and Harnessing Adversarial Examples"}, {"arrows": "to", "from": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples", "to": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"arrows": "to", "from": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples", "to": "Intriguing properties of neural networks"}, {"arrows": "to", "from": "Language Models are Open Knowledge Graphs", "to": "Language Models are Few-Shot Learners"}, {"arrows": "to", "from": "Language Models are Open Knowledge Graphs", "to": "What Does BERT Look At? An Analysis of BERT\u0027s Attention"}, {"arrows": "to", "from": "Language Models are Open Knowledge Graphs", "to": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"}, {"arrows": "to", "from": "Language Models are Open Knowledge Graphs", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Language Models are Open Knowledge Graphs", "to": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"arrows": "to", "from": "Language Models are Open Knowledge Graphs", "to": "Language Models as Knowledge Bases?"}, {"arrows": "to", "from": "Language Models are Open Knowledge Graphs", "to": "How Context Affects Language Models\u0027 Factual Predictions"}, {"arrows": "to", "from": "Meta-Learning for Low-Resource Neural Machine Translation", "to": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"arrows": "to", "from": "Meta-Learning for Low-Resource Neural Machine Translation", "to": "Improving Neural Machine Translation Models with Monolingual Data"}, {"arrows": "to", "from": "Gradient descent aligns the layers of deep linear networks", "to": "Risk and parameter convergence of logistic regression"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "Diverse Neural Network Learns True Target Functions"}, {"arrows": "to", "from": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples", "to": "Explaining and Harnessing Adversarial Examples"}, {"arrows": "to", "from": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "It\u0027s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners", "to": "Distilling the Knowledge in a Neural Network"}, {"arrows": "to", "from": "It\u0027s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "It\u0027s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners", "to": "A Simple Method for Commonsense Reasoning"}, {"arrows": "to", "from": "It\u0027s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners", "to": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"}, {"arrows": "to", "from": "PIQA: Reasoning about Physical Commonsense in Natural Language", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Revisiting Adversarial Risk", "to": "Explaining and Harnessing Adversarial Examples"}, {"arrows": "to", "from": "Revisiting Adversarial Risk", "to": "Intriguing properties of neural networks"}, {"arrows": "to", "from": "Revisiting Adversarial Risk", "to": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"arrows": "to", "from": "Revisiting Adversarial Risk", "to": "Adversarial examples from computational constraints"}, {"arrows": "to", "from": "Pretrained Transformers Improve Out-of-Distribution Robustness", "to": "Gaussian Error Linear Units (GELUs)"}, {"arrows": "to", "from": "A Context-aware Natural Language Generator for Dialogue Systems", "to": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings"}, {"arrows": "to", "from": "A Context-aware Natural Language Generator for Dialogue Systems", "to": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment"}, {"arrows": "to", "from": "Shake-Shake regularization", "to": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"arrows": "to", "from": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets", "to": "Wide Residual Networks"}, {"arrows": "to", "from": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension", "to": "A Simple Method for Commonsense Reasoning"}, {"arrows": "to", "from": "Language Models as Knowledge Bases?", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "A mean-field limit for certain deep neural networks"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "Dynamically Stable Infinite-Width Limits of Neural Classifiers"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "Mean Field Analysis of Deep Neural Networks"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "Feature Learning in Infinite-Width Neural Networks"}, {"arrows": "to", "from": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Self-supervised Learning: Generative or Contrastive", "to": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"}, {"arrows": "to", "from": "Self-supervised Learning: Generative or Contrastive", "to": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"arrows": "to", "from": "Self-supervised Learning: Generative or Contrastive", "to": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"arrows": "to", "from": "Self-supervised Learning: Generative or Contrastive", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Self-supervised Learning: Generative or Contrastive", "to": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"}, {"arrows": "to", "from": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "to": "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time"}, {"arrows": "to", "from": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "to": "Learning Two-layer Neural Networks with Symmetric Inputs"}, {"arrows": "to", "from": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "to": "Learning One Convolutional Layer with Overlapping Patches"}, {"arrows": "to", "from": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "How Context Affects Language Models\u0027 Factual Predictions", "to": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"arrows": "to", "from": "How Context Affects Language Models\u0027 Factual Predictions", "to": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"arrows": "to", "from": "How Context Affects Language Models\u0027 Factual Predictions", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "Stronger generalization bounds for deep nets via a compression approach"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "Size-Independent Sample Complexity of Neural Networks"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "Identity Matters in Deep Learning"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "Gradient descent aligns the layers of deep linear networks"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "Benefits of depth in neural networks"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks", "to": "Learning One-hidden-layer ReLU Networks via Gradient Descent"}, {"arrows": "to", "from": "Learning One Convolutional Layer with Overlapping Patches", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Learning One Convolutional Layer with Overlapping Patches", "to": "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time"}, {"arrows": "to", "from": "Learning One Convolutional Layer with Overlapping Patches", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "Learning One Convolutional Layer with Overlapping Patches", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "A mean-field limit for certain deep neural networks", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "A mean-field limit for certain deep neural networks", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "A mean-field limit for certain deep neural networks", "to": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"arrows": "to", "from": "A mean-field limit for certain deep neural networks", "to": "A Mean Field View of the Landscape of Two-Layers Neural Networks"}, {"arrows": "to", "from": "A mean-field limit for certain deep neural networks", "to": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks"}, {"arrows": "to", "from": "A mean-field limit for certain deep neural networks", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "A mean-field limit for certain deep neural networks", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "A mean-field limit for certain deep neural networks", "to": "Mean Field Analysis of Deep Neural Networks"}, {"arrows": "to", "from": "A mean-field limit for certain deep neural networks", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss", "to": "Risk and parameter convergence of logistic regression"}, {"arrows": "to", "from": "Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss", "to": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"arrows": "to", "from": "When BERT Plays the Lottery, All Tickets Are Winning", "to": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"arrows": "to", "from": "Adversarial Spheres", "to": "Explaining and Harnessing Adversarial Examples"}, {"arrows": "to", "from": "Adversarial Spheres", "to": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"arrows": "to", "from": "Adversarial Spheres", "to": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples"}, {"arrows": "to", "from": "Adversarial Spheres", "to": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models"}, {"arrows": "to", "from": "Adversarial Spheres", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "to": "Benefits of depth in neural networks"}, {"arrows": "to", "from": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "to": "Diverse Neural Network Learns True Target Functions"}, {"arrows": "to", "from": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Mean Field Analysis of Neural Networks: A Law of Large Numbers", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Mean Field Analysis of Neural Networks: A Law of Large Numbers", "to": "Mean Field Analysis of Deep Neural Networks"}, {"arrows": "to", "from": "Mean Field Analysis of Neural Networks: A Law of Large Numbers", "to": "Benefits of depth in neural networks"}, {"arrows": "to", "from": "Mean Field Analysis of Neural Networks: A Law of Large Numbers", "to": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"arrows": "to", "from": "Intriguing properties of neural networks", "to": "Efficient Estimation of Word Representations in Vector Space"}, {"arrows": "to", "from": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Feature Purification: How Adversarial Training Performs Robust Deep Learning"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Learning Two Layer Rectified Neural Networks in Polynomial Time"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Theoretical properties of the global optimizer of two layer neural network"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Learning Two-layer Neural Networks with Symmetric Inputs"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Linearized two-layers neural networks in high dimension"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Finite Depth and Width Corrections to the Neural Tangent Kernel"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Making Method of Moments Great Again? -- How can GANs learn distributions"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Enhanced Convolutional Neural Tangent Kernels"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Diverse Neural Network Learns True Target Functions"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "On the Power and Limitations of Random Features for Understanding Neural Networks"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Learning One-hidden-layer ReLU Networks via Gradient Descent"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Learning Transferable Architectures for Scalable Image Recognition", "to": "Learning to Optimize Neural Nets"}, {"arrows": "to", "from": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "to": "Character-Level Language Modeling with Deeper Self-Attention"}, {"arrows": "to", "from": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "to": "Exploring the Limits of Language Modeling"}, {"arrows": "to", "from": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "to": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"arrows": "to", "from": "A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples", "to": "Explaining and Harnessing Adversarial Examples"}, {"arrows": "to", "from": "A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples", "to": "Intriguing properties of neural networks"}, {"arrows": "to", "from": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Visualizing the Loss Landscape of Neural Nets", "to": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks"}, {"arrows": "to", "from": "Diverse Neural Network Learns True Target Functions", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds", "to": "Learning Two-layer Neural Networks with Symmetric Inputs"}, {"arrows": "to", "from": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds", "to": "Learning One Convolutional Layer with Overlapping Patches"}, {"arrows": "to", "from": "Deep Learning Scaling is Predictable, Empirically", "to": "Exploring the Limits of Language Modeling"}, {"arrows": "to", "from": "Deep Learning Scaling is Predictable, Empirically", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Universal Language Model Fine-tuning for Text Classification", "to": "What makes ImageNet good for transfer learning?"}, {"arrows": "to", "from": "Universal Language Model Fine-tuning for Text Classification", "to": "Improving Neural Machine Translation Models with Monolingual Data"}, {"arrows": "to", "from": "High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks", "to": "Adversarial examples from computational constraints"}, {"arrows": "to", "from": "High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks", "to": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data"}, {"arrows": "to", "from": "High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks", "to": "Intriguing properties of neural networks"}, {"arrows": "to", "from": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "to": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "to": "How to Start Training: The Effect of Initialization and Architecture"}, {"arrows": "to", "from": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics", "to": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks", "to": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data"}, {"arrows": "to", "from": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks", "to": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"arrows": "to", "from": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems", "to": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems", "to": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks"}, {"arrows": "to", "from": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems", "to": "Learning One-hidden-layer ReLU Networks via Gradient Descent"}, {"arrows": "to", "from": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Very Deep Transformers for Neural Machine Translation", "to": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning"}, {"arrows": "to", "from": "Very Deep Transformers for Neural Machine Translation", "to": "Language Models are Few-Shot Learners"}, {"arrows": "to", "from": "Very Deep Transformers for Neural Machine Translation", "to": "Understanding the Difficulty of Training Transformers"}, {"arrows": "to", "from": "Very Deep Transformers for Neural Machine Translation", "to": "Adversarial Training for Large Neural Language Models"}, {"arrows": "to", "from": "Distilling the Knowledge in a Neural Network", "to": "Improving neural networks by preventing co-adaptation of feature detectors"}, {"arrows": "to", "from": "A Mean Field View of the Landscape of Two-Layers Neural Networks", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "A Mean Field View of the Landscape of Two-Layers Neural Networks", "to": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"arrows": "to", "from": "A Mean Field View of the Landscape of Two-Layers Neural Networks", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "A Mean Field View of the Landscape of Two-Layers Neural Networks", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "A Mean Field View of the Landscape of Two-Layers Neural Networks", "to": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks"}, {"arrows": "to", "from": "A Mean Field View of the Landscape of Two-Layers Neural Networks", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "A Mean Field View of the Landscape of Two-Layers Neural Networks", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "A Provably Correct Algorithm for Deep Learning that Actually Works", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "A Provably Correct Algorithm for Deep Learning that Actually Works", "to": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data"}, {"arrows": "to", "from": "A Provably Correct Algorithm for Deep Learning that Actually Works", "to": "Deep Learning and Hierarchal Generative Models"}, {"arrows": "to", "from": "A Provably Correct Algorithm for Deep Learning that Actually Works", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "A Provably Correct Algorithm for Deep Learning that Actually Works", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Learning Parities with Neural Networks", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Learning Parities with Neural Networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Learning Parities with Neural Networks", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "Learning Parities with Neural Networks", "to": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data"}, {"arrows": "to", "from": "Learning Parities with Neural Networks", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Learning Parities with Neural Networks", "to": "A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics"}, {"arrows": "to", "from": "Learning Parities with Neural Networks", "to": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks"}, {"arrows": "to", "from": "Learning Parities with Neural Networks", "to": "Diverse Neural Network Learns True Target Functions"}, {"arrows": "to", "from": "Learning Parities with Neural Networks", "to": "On the Power and Limitations of Random Features for Understanding Neural Networks"}, {"arrows": "to", "from": "Width Provably Matters in Optimization for Deep Linear Neural Networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Width Provably Matters in Optimization for Deep Linear Neural Networks", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Width Provably Matters in Optimization for Deep Linear Neural Networks", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Width Provably Matters in Optimization for Deep Linear Neural Networks", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Width Provably Matters in Optimization for Deep Linear Neural Networks", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Width Provably Matters in Optimization for Deep Linear Neural Networks", "to": "Learning One-hidden-layer ReLU Networks via Gradient Descent"}, {"arrows": "to", "from": "Width Provably Matters in Optimization for Deep Linear Neural Networks", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "Stronger generalization bounds for deep nets via a compression approach"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "Adversarial examples from computational constraints"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "Adversarial Spheres"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "Size-Independent Sample Complexity of Neural Networks"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "Explaining and Harnessing Adversarial Examples"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "A Mean Field View of the Landscape of Two-Layers Neural Networks"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "Revisiting Adversarial Risk"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "Intriguing properties of neural networks"}, {"arrows": "to", "from": "Rademacher Complexity for Adversarially Robust Generalization", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "StereoSet: Measuring stereotypical bias in pretrained language models", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Mean Field Analysis of Deep Neural Networks", "to": "A mean-field limit for certain deep neural networks"}, {"arrows": "to", "from": "Mean Field Analysis of Deep Neural Networks", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Mean Field Analysis of Deep Neural Networks", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Mean Field Analysis of Deep Neural Networks", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Mean Field Analysis of Deep Neural Networks", "to": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"arrows": "to", "from": "Mean Field Analysis of Deep Neural Networks", "to": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks"}, {"arrows": "to", "from": "Mean Field Analysis of Deep Neural Networks", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Mean Field Analysis of Deep Neural Networks", "to": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"arrows": "to", "from": "Learning Two-layer Neural Networks with Symmetric Inputs", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Learning Two-layer Neural Networks with Symmetric Inputs", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Learning Two-layer Neural Networks with Symmetric Inputs", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "Learning Two-layer Neural Networks with Symmetric Inputs", "to": "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time"}, {"arrows": "to", "from": "Learning Two-layer Neural Networks with Symmetric Inputs", "to": "Learning One Convolutional Layer with Overlapping Patches"}, {"arrows": "to", "from": "Learning Two-layer Neural Networks with Symmetric Inputs", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Learning Two-layer Neural Networks with Symmetric Inputs", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$", "to": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data"}, {"arrows": "to", "from": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$", "to": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$", "to": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"arrows": "to", "from": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$", "to": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks"}, {"arrows": "to", "from": "Stabilize Deep ResNet with A Sharp Scaling Factor $\u03c4$", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks", "to": "A mean-field limit for certain deep neural networks"}, {"arrows": "to", "from": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks", "to": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits"}, {"arrows": "to", "from": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks", "to": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks"}, {"arrows": "to", "from": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks", "to": "Mean Field Analysis of Deep Neural Networks"}, {"arrows": "to", "from": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "to": "Exploring the Limits of Language Modeling"}, {"arrows": "to", "from": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "to": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"arrows": "to", "from": "A Simple Method for Commonsense Reasoning", "to": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"arrows": "to", "from": "A Simple Method for Commonsense Reasoning", "to": "Efficient Estimation of Word Representations in Vector Space"}, {"arrows": "to", "from": "A Simple Method for Commonsense Reasoning", "to": "Universal Language Model Fine-tuning for Text Classification"}, {"arrows": "to", "from": "Cross-lingual Language Model Pretraining", "to": "Character-Level Language Modeling with Deeper Self-Attention"}, {"arrows": "to", "from": "Cross-lingual Language Model Pretraining", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Cross-lingual Language Model Pretraining", "to": "Gaussian Error Linear Units (GELUs)"}, {"arrows": "to", "from": "Cross-lingual Language Model Pretraining", "to": "Exploring the Limits of Language Modeling"}, {"arrows": "to", "from": "How to Start Training: The Effect of Initialization and Architecture", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function", "to": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them"}, {"arrows": "to", "from": "Technical report on Conversational Question Answering", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Technical report on Conversational Question Answering", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Technical report on Conversational Question Answering", "to": "Decoupled Weight Decay Regularization"}, {"arrows": "to", "from": "Technical report on Conversational Question Answering", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "to": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys"}, {"arrows": "to", "from": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "to": "Sharp Minima Can Generalize For Deep Nets"}, {"arrows": "to", "from": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "to": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"arrows": "to", "from": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "Linearized two-layers neural networks in high dimension"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "How to Start Training: The Effect of Initialization and Architecture"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "On the Selection of Initialization and Activation Function for Deep Neural Networks"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "A Mean Field Theory of Batch Normalization"}, {"arrows": "to", "from": "Tensor Programs II: Neural Tangent Kernel for Any Architecture", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Learning to Optimize Neural Nets", "to": "Adam: A Method for Stochastic Optimization"}, {"arrows": "to", "from": "Recovery Guarantees for One-hidden-layer Neural Networks", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Recovery Guarantees for One-hidden-layer Neural Networks", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding", "to": "Distilling the Knowledge in a Neural Network"}, {"arrows": "to", "from": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding", "to": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"arrows": "to", "from": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding", "to": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"arrows": "to", "from": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "to": "Identity Matters in Deep Learning"}, {"arrows": "to", "from": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "to": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks"}, {"arrows": "to", "from": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "to": "Benefits of depth in neural networks"}, {"arrows": "to", "from": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "to": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"}, {"arrows": "to", "from": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Learning One-hidden-layer ReLU Networks via Gradient Descent", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Tensor Programs III: Neural Matrix Laws", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Tensor Programs III: Neural Matrix Laws", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Tensor Programs III: Neural Matrix Laws", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Tensor Programs III: Neural Matrix Laws", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Tensor Programs III: Neural Matrix Laws", "to": "How to Start Training: The Effect of Initialization and Architecture"}, {"arrows": "to", "from": "Tensor Programs III: Neural Matrix Laws", "to": "On the Selection of Initialization and Activation Function for Deep Neural Networks"}, {"arrows": "to", "from": "Tensor Programs III: Neural Matrix Laws", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "Tensor Programs III: Neural Matrix Laws", "to": "The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks"}, {"arrows": "to", "from": "Tensor Programs III: Neural Matrix Laws", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Tensor Programs III: Neural Matrix Laws", "to": "A Mean Field Theory of Batch Normalization"}, {"arrows": "to", "from": "Tensor Programs III: Neural Matrix Laws", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "On the Power and Limitations of Random Features for Understanding Neural Networks", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "On the Power and Limitations of Random Features for Understanding Neural Networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "On the Power and Limitations of Random Features for Understanding Neural Networks", "to": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data"}, {"arrows": "to", "from": "On the Power and Limitations of Random Features for Understanding Neural Networks", "to": "Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "On the Power and Limitations of Random Features for Understanding Neural Networks", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "On the Power and Limitations of Random Features for Understanding Neural Networks", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "On the Power and Limitations of Random Features for Understanding Neural Networks", "to": "Linearized two-layers neural networks in high dimension"}, {"arrows": "to", "from": "On the Power and Limitations of Random Features for Understanding Neural Networks", "to": "No bad local minima: Data independent training error guarantees for multilayer neural networks"}, {"arrows": "to", "from": "On the Power and Limitations of Random Features for Understanding Neural Networks", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "UnifiedQA: Crossing Format Boundaries With a Single QA System", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "UnifiedQA: Crossing Format Boundaries With a Single QA System", "to": "The Natural Language Decathlon: Multitask Learning as Question Answering"}, {"arrows": "to", "from": "Dynamically Stable Infinite-Width Limits of Neural Classifiers", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "Dynamically Stable Infinite-Width Limits of Neural Classifiers", "to": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy"}, {"arrows": "to", "from": "Dynamically Stable Infinite-Width Limits of Neural Classifiers", "to": "Mean Field Analysis of Deep Neural Networks"}, {"arrows": "to", "from": "Dynamically Stable Infinite-Width Limits of Neural Classifiers", "to": "A mean-field limit for certain deep neural networks"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "Universal Language Model Fine-tuning for Text Classification"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "Language Models as Knowledge Bases?"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "to": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"}, {"arrows": "to", "from": "On the Selection of Initialization and Activation Function for Deep Neural Networks", "to": "Gaussian Error Linear Units (GELUs)"}, {"arrows": "to", "from": "Deep Learning and Hierarchal Generative Models", "to": "Benefits of depth in neural networks"}, {"arrows": "to", "from": "On the infinite width limit of neural networks with a standard parameterization", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "On the infinite width limit of neural networks with a standard parameterization", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Generating Wikipedia by Summarizing Long Sequences", "to": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"arrows": "to", "from": "Generating Wikipedia by Summarizing Long Sequences", "to": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"arrows": "to", "from": "Story Ending Prediction by Transferable BERT", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Story Ending Prediction by Transferable BERT", "to": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"arrows": "to", "from": "Story Ending Prediction by Transferable BERT", "to": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"arrows": "to", "from": "The large learning rate phase of deep learning: the catapult mechanism", "to": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy"}, {"arrows": "to", "from": "The large learning rate phase of deep learning: the catapult mechanism", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "The large learning rate phase of deep learning: the catapult mechanism", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation", "to": "Universal Sentence Encoder"}, {"arrows": "to", "from": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation", "to": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"}, {"arrows": "to", "from": "Adversarial Training for Large Neural Language Models", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Adversarial Training for Large Neural Language Models", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Adversarial Training for Large Neural Language Models", "to": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"arrows": "to", "from": "Adversarial Training for Large Neural Language Models", "to": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}, {"arrows": "to", "from": "Adversarial Training for Large Neural Language Models", "to": "Adversarial Training Can Hurt Generalization"}, {"arrows": "to", "from": "Adversarial Training for Large Neural Language Models", "to": "Know What You Don\u0027t Know: Unanswerable Questions for SQuAD"}, {"arrows": "to", "from": "Adversarial Training for Large Neural Language Models", "to": "A Simple Method for Commonsense Reasoning"}, {"arrows": "to", "from": "Adversarial Training for Large Neural Language Models", "to": "Neural Network Acceptability Judgments"}, {"arrows": "to", "from": "Adversarial Training for Large Neural Language Models", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "to": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"arrows": "to", "from": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "to": "Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits"}, {"arrows": "to", "from": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "to": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach"}, {"arrows": "to", "from": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "to": "Mean Field Analysis of Neural Networks: A Law of Large Numbers"}, {"arrows": "to", "from": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes", "to": "How to Start Training: The Effect of Initialization and Architecture"}, {"arrows": "to", "from": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes", "to": "A Mean Field Theory of Batch Normalization"}, {"arrows": "to", "from": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference", "to": "Distilling the Knowledge in a Neural Network"}, {"arrows": "to", "from": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference", "to": "The Natural Language Decathlon: Multitask Learning as Question Answering"}, {"arrows": "to", "from": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference", "to": "A Simple Method for Commonsense Reasoning"}, {"arrows": "to", "from": "HellaSwag: Can a Machine Really Finish Your Sentence?", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "TTTTTackling WinoGrande Schemas", "to": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"arrows": "to", "from": "TTTTTackling WinoGrande Schemas", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "TTTTTackling WinoGrande Schemas", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "TTTTTackling WinoGrande Schemas", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Understanding deep learning requires rethinking generalization"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Learning One-hidden-layer Neural Networks with Landscape Design"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Gradient descent aligns the layers of deep linear networks"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "On Lazy Training in Differentiable Programming"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "A Mean Field View of the Landscape of Two-Layers Neural Networks"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"}, {"arrows": "to", "from": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks", "to": "Reconciling modern machine learning practice and the bias-variance trade-off"}, {"arrows": "to", "from": "Multi-Task Deep Neural Networks for Natural Language Understanding", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Multi-Task Deep Neural Networks for Natural Language Understanding", "to": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding"}, {"arrows": "to", "from": "Multi-Task Deep Neural Networks for Natural Language Understanding", "to": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"arrows": "to", "from": "Multilingual Denoising Pre-training for Neural Machine Translation", "to": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"arrows": "to", "from": "Multilingual Denoising Pre-training for Neural Machine Translation", "to": "Cross-lingual Language Model Pretraining"}, {"arrows": "to", "from": "Multilingual Denoising Pre-training for Neural Machine Translation", "to": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"arrows": "to", "from": "Multilingual Denoising Pre-training for Neural Machine Translation", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "Multilingual Denoising Pre-training for Neural Machine Translation", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "Multilingual Denoising Pre-training for Neural Machine Translation", "to": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"arrows": "to", "from": "TinyBERT: Distilling BERT for Natural Language Understanding", "to": "Distilling the Knowledge in a Neural Network"}, {"arrows": "to", "from": "TinyBERT: Distilling BERT for Natural Language Understanding", "to": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"arrows": "to", "from": "TinyBERT: Distilling BERT for Natural Language Understanding", "to": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"arrows": "to", "from": "TinyBERT: Distilling BERT for Natural Language Understanding", "to": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"arrows": "to", "from": "Defending Against Neural Fake News", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "A mean-field limit for certain deep neural networks"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "On Exact Computation with an Infinitely Wide Neural Net"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "Linearized two-layers neural networks in high dimension"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "Mean Field Analysis of Deep Neural Networks"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "On the Power and Limitations of Random Features for Understanding Neural Networks"}, {"arrows": "to", "from": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "The Woman Worked as a Babysitter: On Biases in Language Generation", "to": "Exploring the Limits of Language Modeling"}, {"arrows": "to", "from": "Know What You Don\u0027t Know: Unanswerable Questions for SQuAD", "to": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"arrows": "to", "from": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "to": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"arrows": "to", "from": "Automatic Detection of Generated Text is Easiest when Humans are Fooled", "to": "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"arrows": "to", "from": "Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs", "to": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"arrows": "to", "from": "Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "A Mean Field Theory of Batch Normalization", "to": "Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs"}, {"arrows": "to", "from": "A Mean Field Theory of Batch Normalization", "to": "How to Start Training: The Effect of Initialization and Architecture"}, {"arrows": "to", "from": "A Mean Field Theory of Batch Normalization", "to": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"arrows": "to", "from": "A Mean Field Theory of Batch Normalization", "to": "The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks"}, {"arrows": "to", "from": "A Mean Field Theory of Batch Normalization", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality", "to": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"arrows": "to", "from": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality", "to": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"}, {"arrows": "to", "from": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality", "to": "Countering Adversarial Images using Input Transformations"}, {"arrows": "to", "from": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality", "to": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"arrows": "to", "from": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality", "to": "Towards moderate overparameterization: global convergence guarantees for training shallow neural networks"}, {"arrows": "to", "from": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality", "to": "Intriguing properties of neural networks"}, {"arrows": "to", "from": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}, {"arrows": "to", "from": "Kernel and Rich Regimes in Overparametrized Models", "to": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"arrows": "to", "from": "Kernel and Rich Regimes in Overparametrized Models", "to": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"arrows": "to", "from": "Kernel and Rich Regimes in Overparametrized Models", "to": "Gradient Descent Finds Global Minima of Deep Neural Networks"}, {"arrows": "to", "from": "Kernel and Rich Regimes in Overparametrized Models", "to": "Disentangling feature and lazy training in deep neural networks"}, {"arrows": "to", "from": "Kernel and Rich Regimes in Overparametrized Models", "to": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"arrows": "to", "from": "Kernel and Rich Regimes in Overparametrized Models", "to": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "barnesHut": {
            "avoidOverlap": 0,
            "centralGravity": 0.3,
            "damping": 0.09,
            "gravitationalConstant": -80000,
            "springConstant": 0.001,
            "springLength": 250
        },
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>