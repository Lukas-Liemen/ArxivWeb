{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mperZlxBpouK"
      },
      "outputs": [],
      "source": [
        "!pip install arxiv\n",
        "!pip install pyvis\n",
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import fitz\n",
        "from collections import defaultdict\n",
        "import arxiv\n",
        "import threading\n",
        "import concurrent.futures\n",
        "from pyvis.network import Network\n",
        "\n",
        "\n",
        "MAX_CONNECTIONS = 50\n",
        "client = arxiv.Client()\n",
        "\n",
        "\n",
        "def search_paper(ids, max_results=10):\n",
        "    search = arxiv.Search(id_list=ids)\n",
        "    results = list(client.results(search))\n",
        "    return results\n",
        "\n",
        "\n",
        "def extract_text_from_pdf_url(pdf_url):\n",
        "    response = requests.get(pdf_url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    doc = fitz.open(stream=response.content, filetype=\"pdf\")\n",
        "\n",
        "    text = []\n",
        "    for page_num in range(doc.page_count):\n",
        "        page = doc.load_page(page_num)\n",
        "        text.extend([t[4] for t in page.get_text(\"blocks\") if \"arxiv:\" in t[4].lower()])\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_referenced_papers(text):\n",
        "    referenced = []\n",
        "    for t in text:\n",
        "        id_text = t.lower().split(\"arxiv:\")[1].replace(\" \", \"\")[:10]\n",
        "        referenced.append(id_text)\n",
        "    return referenced\n",
        "\n",
        "\n",
        "def rec_references(id, depth=0, max_depth=1, referenced_papers={}, title_map={}):\n",
        "    if depth > max_depth:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        results = search_paper([id])\n",
        "\n",
        "        # Process the paper\n",
        "        for r in results:\n",
        "            title_map[id] = r.title\n",
        "            pdf_url = [url for url in r.links if url.title == \"pdf\"][0]\n",
        "            text = extract_text_from_pdf_url(pdf_url)\n",
        "            referenced_papers[id] = extract_referenced_papers(text)\n",
        "\n",
        "        # Use a thread pool to limit the number of concurrent threads\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_CONNECTIONS) as executor:\n",
        "            futures = []\n",
        "            for paper in referenced_papers[id]:\n",
        "                futures.append(executor.submit(rec_references, paper, depth+1, max_depth, referenced_papers, title_map))\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                future.result()\n",
        "\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    return referenced_papers, title_map\n",
        "\n",
        "\n",
        "def create_network(reference_dict, title_map):\n",
        "    net = Network(bgcolor=\"#222222\", font_color=\"white\", directed=True)\n",
        "    net.barnes_hut()\n",
        "\n",
        "    # Create a dictionary to store the in-degree of each node\n",
        "    in_degree = {title: 0 for title in title_map.values()}\n",
        "\n",
        "    # Calculate the in-degree of each node\n",
        "    for paper_id, references in reference_dict.items():\n",
        "        for ref_id in references:\n",
        "            try:\n",
        "                ref_title = title_map[ref_id]\n",
        "                in_degree[ref_title] += 1\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    # Add nodes with size based on in-degree\n",
        "    for paper_id in reference_dict:\n",
        "        title = title_map[paper_id]\n",
        "        size = in_degree[title] * 5\n",
        "        net.add_node(title, size=size, title=title)\n",
        "\n",
        "    # Add edges\n",
        "    for paper_id, references in reference_dict.items():\n",
        "        for ref_id in references:\n",
        "            try:\n",
        "                title = title_map[paper_id]\n",
        "                ref_title = title_map[ref_id]\n",
        "                if title != ref_title:\n",
        "                    net.add_edge(title, ref_title)\n",
        "            except AssertionError:\n",
        "                continue\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "7UbXUX315s3T"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id = \"2106.09685\  # Change to the ID of the paper you're interested in"\n",
        "file_name = \"Papers.html\"\n",
        "\n",
        "referenced_papers, title_map = rec_references(id, max_depth=2)\n",
        "\n",
        "net = create_network(referenced_papers, title_map)\n",
        "net.show(file_name, notebook=False)"
      ],
      "metadata": {
        "id": "vIUZ-3fAtvqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
